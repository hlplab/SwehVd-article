---
title: "Evaluating normalization accounts against the dense vowel space of Central Swedish"
runtitle: "Evaluating normalization accounts"
documentclass: frontiersSCNS  # or frontiersHLTH, or frontiersFPHY
author:
  - name: Anna S. Persson
    affiliation: '1'
    etal: Persson # First author's last name.
    email: anna.persson@su.se # Indicates corresponding Author
    institution: Department of Swedish Language and Multilingualism
    street: Stockholm University
    city: Stockholm
    zip: SE-106 91
    country: Sweden
  - name: T. Florian Jaeger
    affiliation: '2,3'

affiliation:
  - id: '1'
    department: Department of Swedish Language and Multilingualism
    institution: Stockholm University
    city: Stockholm
    country: Sweden
  - id: '2'
    department: Brain and Cognitive Sciences
    institution: University of Rochester
    city: Rochester
    state: NY # only USA, Australia, Canada
    country: USA
  - id: '3'
    department: Computer Science
    institution: University of Rochester
    city: Rochester
    state: NY # only USA, Australia, Canada
    country: USA

csl               : frontiers.csl
link-citations    : yes
bibliography      : ["latex-stuff/library.bib", "latex-stuff/r-references.bib"]

header-includes:
 - \usepackage{colortbl}
 - \usepackage{lscape}
 - \usepackage{pdflscape}
 - \usepackage{tablefootnote}

output:
  bookdown::pdf_book:
    base_format: rticles::frontiers_article
    latex_engine: xelatex
    includes:
      in_header: latex-stuff/header-frontiers.tex
---

\begin{abstract}
Talkers vary in the phonetic realization of their vowels. One influential hypothesis holds that listeners overcome this inter-talker variability through pre-linguistic auditory mechanisms that normalize the acoustic or phonetic cues that form the input to speech recognition. Dozens of competing normalization accounts exist---including both accounts specific to vowel perception and general purpose accounts that can be applied to any type of cue. We add to the cross-linguistic literature on this matter by comparing normalization accounts against a new phonetically annotated vowel database of Swedish, a language with a particularly dense vowel inventory of 21 vowels differing in quality and quantity. We evaluate normalization accounts on how they differ in predicted consequences for perception. The results indicate that the best performing accounts either center or standardize formants by talker. The study also suggests that general purpose accounts perform as well as vowel-specific accounts, and that vowel normalization operates in both temporal and spectral domains.

\tiny
 \keyFont{ \section{Keywords:} vowel normalization; category separability; ideal observers; speech production; speech perception}

\end{abstract}

```{r libraries, include=FALSE, message=FALSE}
library(curl)             # Check availability of internet for install of remote libraries

# Install remotes package if necessary
if(!requireNamespace("remotes", quietly = TRUE)) if (has_internet()) install.packages("remotes")
# Install the stable development version from GitHub
if (has_internet()) remotes::install_github("crsh/papaja")
library(papaja)           # APA formatted ms

library(tidyverse)        # data wrangling and plotting
library(magrittr)         # pipes, my friend, we need pipes
library(assertthat)       # for error checking
library(rlang)            # quosures and unquoting
library(ggforce)          # correlation matrices
library(cowplot)
library(plotly)           # for interactive HTML plots
library(mvtnorm)          # multivariate Gaussian distributions

if (has_internet()) remotes::install_github("hlplab/MVBeliefUpdatr")
library(MVBeliefUpdatr)   # for generating Ideal Observers

library(linguisticsdown)  # for inserting IPA symbols
library(kableExtra)       # for styling html format
library(modelr)
library(phonR)            # for vowel normalization functions
library(phonTools)        # for WattFabricius normalization function
```

```{r setup, include=FALSE}
library(knitr)

# Set knit defaults for code chunks
opts_chunk$set(
  dpi=300,
  dev='jpeg', # default format of figures
  comment="",
  echo=FALSE, warning=FALSE, message=FALSE,
  cache=TRUE)

# some useful formatting functions for output of knitting
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}', color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

```{r functions-and-pipes, include=FALSE}
source("constants.R")

INCLUDE_TODO = F     # switch on/off to get to do list.
INCLUDE_SI = T     # switch on/off to include SI.

myGplot.defaults("paper")

options(knitr.table.format = "latex")

base.width = 2.5
base.height = 2.5
base_size = 10
```

```{r, child="TO-DO.Rmd", eval= if (INCLUDE_TODO) TRUE else FALSE}
```

\newpage
\setcounter{page}{1}

# Introduction {#sec:intro -}
Talkers differ in their pronunciation of individual speech sounds due to both physiological differences and socio-cultural factors, including style, regional dialect, and second language accents. For listeners, this means that the mapping from acoustic cues to linguistic categories---phonemes, syllables, words, and ultimately word meanings---varies depending on the talker. How listeners manage to typically understand talkers despite this "lack of invariance" [@liberman1967] has remained one of the central questions for research on speech perception. Hypotheses about the mechanisms underlying this ability can be grouped into three, mutually compatible and complementary, accounts: (1) low-level, pre-linguistic auditory transformation of the acoustic signal, (2) learning of changes in the linguistic representations, and (3) post-linguistic changes in decision-making biases [see e.g., @johnson2006; @pardo2006; @xie2023]. The present study focuses on the first type of account, that the acoustic signal is transformed and normalized early on during auditory processing [for recent reviews, @johnson-sjerps2021; @stilp2020].

Accounts of pre-linguistic normalization are motivated by *a priori* considerations about both the physics of sounds [cf. the discussion of uniform scaling in @barreda2020a] and evolutionary arguments [e.g., even non-human animals exhibit similar abilities, @barreda2020a]. They are also supported by brain imaging evidence: talker-normalized information about the speech signal can be decoded from areas as early as the brain stem [e.g., @skoe2021auditory], and thus prior to even the earliest cortical areas typically associated with linguistic category representations or decision-making. While it is rather uncontroversial that normalization is part of adaptive speech perception, questions remain about the specific nature of the operations involved in normalization. We contribute to this line of research by comparing different types of normalization accounts against vowel production data from a new phonetically annotated database of Central Swedish vowels (the SwehVd database).

Normalization accounts were originally proposed as a theory of how the brain removes *physiologically*-caused variation from the speech signal [e.g., @Bladon1984; @gerstman1968; @lobanov1971; @miller1989c; @nearey1978; @nordstrom1975; @peterson1961; @Syrdal1986; @sussman1986]. Much of this early work focused specifically on differences in formants, the primary cues the perception of vowel quality. These formants---peaks in the energy distribution over frequencies---are affected by talkers' vocal tract size  [e.g., @fox1995; @Peterson1952; @Verbrugge1977; @yang2014]. Successful normalization was meant to account for these physiological differences, thereby reducing inter-talker variability in the phonetic realization of vowels (compare Figure \@ref(fig:illustrate-normalization)B to \@ref(fig:illustrate-normalization)A), which can can result in reduced category overlap (compare Figure \@ref(fig:illustrate-normalization)D to \@ref(fig:illustrate-normalization)C).^[We note that this argument assumes that listeners' category representations pool experiences across talkers into a single talker-independent model. Such talker-independent category representations are assumed in many influential models of spoken word recognition [e.g., @luce-pisoni1998; @mcclelland-elman1986; @norris-mcqueen2008]. While talker-independent representations might be a simplifying assumption for some of these theories, this assumption has persisted for decades [e.g. @magnuson2020; @tenbosch2022]. Exceptions include exemplar accounts [e.g., @johnson1997; @pierrehumbert2001] and the Bayesian ideal adaptor account [@kleinschmidt-jaeger2015]. Importantly, it is an unresolved question whether---or for which cues and phonetic contrasts---listeners maintain talker-specific *category* representations [for findings and discussion, see @kraljic-samuel2007; @kleinschmidt-jaeger2015; @kleinschmidt2019; @xie2021cognition]. Here, we follow previous work and compare the effectiveness of normalization under the assumption of talker-independent category representations.]

(ref:illustrate-normalization) Illustrating how normalization reduces category overlap for the 8 monophthongs of L1 US English. Three talkers from the @xie2020asa database are shown before (**Panel A**) and after Lobanov normalization [@lobanov1971]---one of the most commonly applied accounts (**Panel B**). Lobanov normalization reduces inter-talker variability in the category means and, to some extent, in the category variances. The bottom two panels aggregate the data from all 17 talkers in the database (5 female, 12 male), showing the means and 95% probability mass bivariate Gaussian densities for each vowel before (**Panels C**) and after Lobanov normalization (**Panel D**).

```{r}
# Load native vowel data from Li & Xie database
d.Eng.LiXie <-
  read_csv('../../data/phonetic vowel statistics/English/Li_Xie_2020_L1_vowels_statistics_general.csv') %>%
    #remove variables related to Miller's height backness space to avoid confusion
    select(-F0_gm) %>%
    mutate(Language = "L1 English",
           Vowel = factor(
             plyr::mapvalues(
               Vowel,
               levels.vowel.Arpabet,
               levels.vowel.IPA),
             levels = levels.vowel.IPA)) %>%
  rename(category = Vowel, F0 = F0.mean, F1 = F1_gm, F2 = F2_gm, F3 = F3_gm) %>%
  relocate(F0, .before = F1) %>%
  group_by(Talker, category) %>%
  #Since the normalization and transformation functions does not take NAs, we replace NAs with the average F0 value for that Talker and category
  mutate(
    F0 = replace_na(F0, geometric.mean(F0)))
```

```{r}
set.seed(333421866)

d.Eng.LiXie.long <-
  d.Eng.LiXie %>%
  # Split data into five equally sized bins  
  split_data() %>%
  # Make cross-validation folds
  crossing(crossvalidation_group = 1:5) %>%
  mutate(fold_type = ifelse(crossvalidation_group != (fold + 3) %% 5 + 1, "training", "test")) %>%
  arrange(crossvalidation_group)

apply_all_transformations_and_normalization <-
  function(data) {
    data %>%
      get_transformation() %>%
      get_normalization_functions(
        data = .,
        normalize_based_on_fold_types = "training")() %>%
      rename(F0_Hz = F0, F1_Hz = F1, F2_Hz = F2, F3_Hz = F3) %>%
      add_C_CuRE(
        data = .,
        cues = c("F0_Hz", "F1_Hz", "F2_Hz", "F3_Hz", "F0_Mel", "F1_Mel", "F2_Mel", "F3_Mel", "F0_ERB", "F1_ERB", "F2_ERB", "F3_ERB", "F0_Bark", "F1_Bark", "F2_Bark", "F3_Bark", "F0_semitones", "F1_semitones", "F2_semitones", "F3_semitones", "Duration"),
        normalize_based_on_fold_types = "training")() %>%
      # Add '_r' for 'raw' to columns with scale-transformed data in order for pivoting to work in next chunk
      rename_with(.fn = ~ paste(.x, "r", sep = "_"), .cols = ends_with(c("Hz", "Mel", "Bark", "ERB", "semitones")))
  }

# Normalize all data based on the respective training of each cross-validation group
d.Eng.LiXie.long %<>%
  group_by(crossvalidation_group) %>%
  # Get transformations, apply normalizations (both classic formants one and C-CuRE)
  group_map(
    .f = ~ apply_all_transformations_and_normalization(data = .x),
    .keep = T
  ) %>%
  reduce(bind_rows) %>%
  ungroup() %>%
  pivot_longer(
    cols = starts_with("F", ignore.case = FALSE),
    names_to = c("Cue", "Normalization.Scale", "Normalization.Type"),
    names_sep = "_",
    values_to = "Cue.Value") %>%
  pivot_wider(
    names_from = "Cue",
    values_from = "Cue.Value") %>%
    # Mutate both
  mutate(Normalization.Type = paste(Normalization.Type, Normalization.Scale, sep = "_")) %>%
  select(-Normalization.Scale) %>%
  relocate(Talker, Normalization.Type, crossvalidation_group, fold_type, fold, category, Token)

p.Eng.LiXie.raw <-
  d.Eng.LiXie.long %>%
  filter(fold_type == "test", Normalization.Type == "r_Hz", Talker %in% c("E1", "E4", "E9")) %>%
  ggplot(
    aes(
      x = F2,
      y = F1)) +
  geom_point(
    aes(
      colour = category),
    alpha = .3) +
  # Add category means
  geom_point(
    data =
      ~ .x %>%
      group_by(category, Talker) %>%
      summarise(across(c(F1, F2), ~ mean(.x))),
    mapping = aes(
      colour = category),
    alpha = .8) +
  scale_color_brewer("Vowel", palette = "Paired") +
  scale_x_reverse("F2 (Hertz)", position = "top", breaks = scales::pretty_breaks(n = 3), limits = c(3100, 740)) +
  scale_y_reverse("F1 (Hertz)", breaks = scales::pretty_breaks(n = 3), limits = c(1100, 200)) +
  guides(color = "none") +
  facet_wrap(~ factor(Talker, levels = c("E1", "E4", "E9"), labels = c("Talker E1", "Talker E4", "Talker E9")), ncol = 1) +
  theme(axis.text = element_text(size = 7),
        axis.title.y = element_text(vjust = 0))

p.Eng.LiXie.Lobanov <-
  p.Eng.LiXie.raw %+%
  (d.Eng.LiXie.long %>%
  filter(fold_type == "test", Normalization.Type == "Lobanov_Hz", Talker %in% c("E1", "E4", "E9"))) +
  scale_x_reverse("F2 (Lobanov)", position = "top", breaks = scales::pretty_breaks(n = 3), limits = c(2.55, -1.8)) +
  scale_y_reverse("F1 (Lobanov)", breaks = scales::pretty_breaks(n = 3), limits = c(2, -2))

p.Eng.LiXie.all.raw <-
  d.Eng.LiXie.long %>%
  filter(fold_type == "test", Normalization.Type == "r_Hz") %>%
  ggplot(
    aes(
      x = F2,
      y = F1,
      color = category)) +
  stat_ellipse() +
  geom_point(
    data =
      ~ .x %>%
      group_by(category) %>%
      summarise(across(c(F1, F2), ~ mean(.x))),
    mapping = aes(
      colour = category),
    alpha = .8) +
  scale_color_brewer("Vowel", palette = "Paired") +
  scale_y_reverse("", breaks = scales::pretty_breaks(n = 3), limits = c(1100, 200)) +
  scale_x_reverse() +
  facet_wrap(~ factor(Normalization.Type, levels = c("r_Hz", "Lobanov_Hz"), labels = c("all talkers", "all talkers")), scales = "free") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank())

p.Eng.LiXie.all.Lob <-
  p.Eng.LiXie.all.raw %+%
  (d.Eng.LiXie.long %>%
     filter(fold_type == "test", Normalization.Type == "Lobanov_Hz")) +
  scale_y_reverse("", breaks = scales::pretty_breaks(n = 3), limits = c(2, -2)) +
  guides(color = "none")

#Get legend from final plot
legend <-
  get_legend(
    # create some space to the left of the legend
    p.Eng.LiXie.all.raw +
      guides(color = guide_legend(nrow = 1, byrow = TRUE)) +
      theme(legend.direction = "horizontal",
            legend.justification="center",
            legend.box.just = "top"))
```

```{r illustrate-normalization, fig.width=base.width*2.5, fig.height=base.height*3.5+.5, out.width='75%', fig.align='center', fig.cap="(ref:illustrate-normalization)"}
plot_grid(
  legend,
  plot_grid(
    plot_grid(p.Eng.LiXie.raw, p.Eng.LiXie.all.raw + theme(legend.position = "none"), labels = c("A", "C"), ncol = 1, align = "hv", rel_heights = c(3/4, 1/4)),
    NULL,
    plot_grid(
      p.Eng.LiXie.Lobanov, p.Eng.LiXie.all.Lob, labels = c("B", "D"), ncol = 1, align = "hv", rel_heights = c(3/4, 1/4)),
    ncol = 3, align = "hv", rel_widths = c(1, -.05, .95)),
    align = "hv", axis = "bltr", nrow = 2, rel_heights = c(.05, .95))
```

Over the decades, dozens of competing accounts of vowel normalization have been proposed [e.g., @Bladon1984; @fant1975; @gerstman1968; @joos1948; @lobanov1971; @miller1989c; @nearey1978; @nordstrom1975; @Syrdal1986; @traunmuller1981; @watt2002; @zahorian1991; for reviews, see @barreda2020a; @weatherholtz-jaeger2016]. @carpenter1993 summarize over 100 different vowel-specific accounts, though---as we discuss later in more detail---many of them share the same basic operations. More recently, additional *general* normalization accounts have emerged that can be applied to *any* type of cue and phonological contrast, rather than just vowel formants [e.g., @cole2010; @mcmurray-jongman2011]. The most widely used of these proposals, C-CuRE, has since been successfully applied to the categorization of US English fricatives [@apfelbaum2014; @crinnion2020; @mcmurray-jongman2011], stop voicing [@kulikov2022; @toscano2015; @xie2023], sentence-final rising question vs. statement intonation [@xie2021cognition], as well as vowels [@kleinschmidt2019; @mcmurray-jongman2016]. In each of these studies, C-CuRE reduced inter-talker variability and improved categorization. C-CuRE, which stands for **c**omputing **cu**es **r**elative to **e**xpectations, captures the motivation behind earlier normalization accounts that the acoustic-phonetic properties of the current speech input should be interpreted relative to their expected distribution in the present context. Unlike many of these earlier accounts, however, C-CuRE is not just meant account for expectations based on talkers' *physiology* but applies equally to expectations based on, for example, talkers' social identity or language background. This makes C-CuRE a potential candidate mechanism for adaptive speech perception beyond physiologically effects on vowel formants, and is the reason we include it in our comparison of normalization accounts.

## The present study {-}
Table \@ref(tab:norm-accounts) lists the normalization accounts investigated in the present study. This includes both the most influential vowel-specific normalization accounts that have been found to perform well in previous works (e.g., Lobanov and Nearey2 normalization) and several variants of the general purpose normalization C-CuRE. As indicated through shading in the table, the accounts can be grouped into four types based on the computational assumptions they make. *Transformations* are meant to transform the formant data from acoustic (Hz) into a perceptual space that approximates the perceptual organization of auditory information in the human brain. All other accounts instead or additionally adjust each formant value based on either the values of other formants on the same segment (*vowel-intrinsic* approaches) or summary statistics of the formant across segments (*vowel-extrinsic* approaches).^[Miller's formant-ratio account [@miller1989c] is technically a hybrid approach: the first formant (F1) is normalized with regard to an extrinsic sensory reference (based on the average F0 across segments); subsequent formants are (intrinsicly) normalized using the normalized lower formants on the same vowel segment.] We further distinguish two types of vowel-extrinsic approaches that differ in their computational complexity and tractability: approaches that *center* each cue relative to its mean across all vowel segments, and approaches that instead/additionally *standardize* cues relative to the overall variability or range of the cue across all vowel segments [for reviews, see also e.g., @Johnson2005c; @kohn2012a; @weatherholtz-jaeger2016].^[Here we group accounts based on their computational complexity (the number of parameters listeners are assumed to estimate). For example, we group Nearey1 and Nearey2 with the centering accounts because they require estimation of only cue means. However, since these accounts perform centering over log-transformed Hz, they can also be considered as a form of functionally constrained standardization in non-log space [@barreda2018a].] The former type includes C-CuRE, and we consider different variants of this approach, one for each transformation approach in Table \@ref(tab:norm-accounts).

The selection of accounts we consider in the present study is primarily based on their influence and performance in previous evaluations against other data sets. Additionally, we only consider accounts that are sufficiently general in nature to be applied across languages. This decision stems from our goal to understand the mechanisms underlying *human* speech perception. This means that we for instance do not include Watt & Fabricius [@watt2002; @fabricius2009], as this account requires specific assumptions of vowel inventories of the language. Finally, we do not consider *combinations* of accounts. This follows the majority of previous work but is an important limitation that we return to in the general discussion.

\begin{landscape}\begin{table}
\caption{\label{tab:norm-accounts}Normalization accounts considered in the present study. Unless otherwise marked, formant variables ($F$s) in the right-handside of normalization formulas are in Hz.}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{>{\arraybackslash}p{0.1cm}>{\arraybackslash}p{0.2cm}>{\arraybackslash}p{0.5cm}|>{\raggedright\arraybackslash}p{3cm}|>{\raggedright\arraybackslash}p{3cm}|>{\raggedright\arraybackslash}p{4cm}|>{\raggedright\arraybackslash}p{6.2cm}}
\hline
& & & Normalization procedure & Perceptual scale & Source & Formula\\
\hline
\hline
& & \cellcolor[HTML]{100C08}{} & none & Hz & n/a & n/a \\

\hline

& & \cellcolor[HTML]{C9C0BB}{} & \cellcolor[HTML]{C9C0BB}{none} & \cellcolor[HTML]{C9C0BB}{Bark} & \cellcolor[HTML]{C9C0BB}{Traunmüller (1990)} & \cellcolor[HTML]{C9C0BB}{$F_n^{Bark} = \frac{26.81 \times F_n}{1960 + F_n} - 0.53$} \\
& & \cellcolor[HTML]{C9C0BB}{} & \cellcolor[HTML]{C9C0BB}{---} & \cellcolor[HTML]{C9C0BB}{ERB}  & \cellcolor[HTML]{C9C0BB}{Glasberg \& Moore (1990)} & \cellcolor[HTML]{C9C0BB}{$F_n^{ERB} = 21.4 \times \log_{10}(1 + F_n \times 0.00437)$} \\
\multirow[c]{-2}{*}{\rotatebox{90}{trans-}} & & \cellcolor[HTML]{C9C0BB}{} & \cellcolor[HTML]{C9C0BB}{---} & \cellcolor[HTML]{C9C0BB}{Mel}  & \cellcolor[HTML]{C9C0BB}{Stevens \& Volkmann (1940)} & \cellcolor[HTML]{C9C0BB}{$F_n^{Mel} = 2595 \times \log_{10}(1 + \frac{F_n}{700})$} \\
& \multirow[c]{-4}{*}{\rotatebox{90}{formation}} & \cellcolor[HTML]{C9C0BB}{} & \cellcolor[HTML]{C9C0BB}{---} & \cellcolor[HTML]{C9C0BB}{Semitones conversion} & \cellcolor[HTML]{C9C0BB}{Fant et al. (2002)} & \cellcolor[HTML]{C9C0BB}{$F_n^{ST} = 12 \times \frac{ln(\frac{F_n}{100})}{ln}$} \\

\hline

& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{Syrdal \& Gopal's} & \cellcolor[HTML]{E6BE8A}{Bark} & \cellcolor[HTML]{E6BE8A}{Syrdal \& Gopal (1986)} & \cellcolor[HTML]{E6BE8A}{$F1^{SyrdalGopal} = F1^{Bark} - F0^{Bark}$} \\
& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{Bark-distance model\tablefootnote{Previous work has considered two different implementations of Syrdal \& Gopal's Bark-distance model for F2, depending on the language (Adank, 2003; Fant, 1983; Syrdal \& Gopal, 1986). In the SI (see Section Evaluation of implementations of Syrdal \& Gopal), we compare these two implementations, and find that the F2-F1 implementation performs better for the present data. We thus present that version of Syrdal \& Gopal's model in the main text.}} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{$F2^{SyrdalGopal} = F2^{Bark} - F1^{Bark}$} \\
& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{Miller} & \cellcolor[HTML]{E6BE8A}{log} & \cellcolor[HTML]{E6BE8A}{Miller (1989)} & \cellcolor[HTML]{E6BE8A}{$SR = k (\frac{GM f0}{k})^{1/3}$} \\
& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{(formant-ratio)} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{$F1^{Miller} = log(\frac{F1}{SR})$} \\
& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{$F2^{Miller} = log(\frac{F2}{F1})$} \\
{\multirow[c]{-7}{*}{\rotatebox{90}{intrinsic}}} & & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{$F3^{Miller} = log(\frac{F3}{F2})$} \\

\hline

& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{C-CuRE} & \cellcolor[HTML]{ABCDEF}{Hz} & \cellcolor[HTML]{ABCDEF}{McMurray \& Jongman (2011)} & \cellcolor[HTML]{ABCDEF}{$F^{C-CuRE}_n = F_n - mean(F_n)$} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{---} & \cellcolor[HTML]{ABCDEF}{Bark} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{---} & \cellcolor[HTML]{ABCDEF}{ERB} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{---} & \cellcolor[HTML]{ABCDEF}{Mel} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{---} & \cellcolor[HTML]{ABCDEF}{Semitones conversion} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\

& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{Nearey1} & \cellcolor[HTML]{ABCDEF}{log} & \cellcolor[HTML]{ABCDEF}{Nearey (1978)} & \cellcolor[HTML]{ABCDEF}{$F^{Nearey1}_n = \ln(F_n) - mean(ln(F_n))$} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{(log-mean)} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\

& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{Nearey2} & \cellcolor[HTML]{ABCDEF}{log} & \cellcolor[HTML]{ABCDEF}{Nearey (1978)} & \cellcolor[HTML]{ABCDEF}{$F^{Nearey2}_n = \ln(F_n) - mean(ln(F))$} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{(single parameter} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{{\multirow[c]{-10}{*}{\rotatebox{90}{centering}}}} & \cellcolor[HTML]{ABCDEF}{log-mean)} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\

\cline{3-6}

& & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{Gerstman} & \cellcolor[HTML]{DDADAF}{Hz} & \cellcolor[HTML]{DDADAF}{Gerstman (1968)} & \cellcolor[HTML]{DDADAF}{$F_n^{Gerstman} = 999 \times \frac{F_n - F_n^{min}}{F_n^{max} - F_n^{min}}$} \\
& & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{(range normalization)} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} \\
& & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} \\

{\multirow[c]{-16}{*}{\rotatebox{90}{extrinsic}}} & & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{Lobanov} & \cellcolor[HTML]{DDADAF}{Hz} & \cellcolor[HTML]{DDADAF}{Lobanov (1971)} & \cellcolor[HTML]{DDADAF}{$F^{Lobanov}_n = \frac{F_n - mean(F_n)}{sd(F_n)}$} \\
& & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{(z-score)} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} \\
& & \cellcolor[HTML]{DDADAF}{{\multirow[c]{-6}{*}{\rotatebox{90}{standardizing}}}} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} \\


\hline
\end{tabular}
\end{table}

\end{landscape}

Existing evaluations of normalization accounts can be broadly grouped into two types: studies that compare accounts in terms of their effectiveness in reducing inter-talker variability in the phonetic realization of categories (Table \@ref(tab:norm-evaluation-variability)), and studies that compare accounts in terms of their expected consequences for perception (Table \@ref(tab:norm-evaluation-percep)). While the two approaches have often yielded similar results, they measure different aspects, and do not *have to* agree. As we show in the SI (\@ref(sec:auxiliaryStudy)), measures of between- vs. within-category separability/variability have downsides that can lead to misleading results. Simply put, reduction of variance is not the ultimate goal of speech perception, and reduced variance does not always result in improved perception. We thus focus on the second approach, as our ultimate interest is in evaluating normalization as a hypothesis about the mechanisms underlying adaptive speech perception.^[\@ref(sec:auxiliaryStudy) in the SI repeats all comparisons presented in the main text but for a measure of category variability/separability, rather than a model of perception. Where relevant, the main text points to differences in the results between these two approaches.] We note, however, that the present study is limited to evaluating the *predicted* consequences for perception, rather than the *fit* of different normalization accounts against perception data. This limitation is shared with the majority of previous work---very few studies to date have compared normalization against listeners' responses in perception experiments [@barreda2021; @mcmurray-jongman2016; @nearey1989; @richter2017]. We return to this important caveat in the discussion.

Several generalizations emerge from Tables \@ref(tab:norm-evaluation-variability) and \@ref(tab:norm-evaluation-percep). First, transformation of the acoustic input to a perceptual scale alone are not particularly effective at reducing variability or improving recognition [see also @adank2004; @carpenter1993; @clopper2009; @escudero2007; @Flynn2011; @kohn2012a]. Accounts that additionally apply intrinsic or extrinsic normalization perform significantly better. In particular, extrinsic normalization accounts that center and/or standardize formants seem to perform best both in reducing inter-talker variability [see e.g. @barreda2018a; @disner1980; @fabricius2009; @kohn2012a; @labov2010; @lobanov1971] and in improving recognition [e.g., @adank2004; @escudero2007; @johnson-sjerps2021; @syrda1985]. When Lobanov and Gerstman normalization---both involving standardizing---were included in a study, they often rank among the top two performing accounts. Of note, Nearey normalization [@nearey1978] often performs well even though it does not involve the computationally more complex operation of standardizing. This suggests that simple centering of formants relative to the talker's mean *might* be sufficient to achieve significant variance reduction [but see @disner1980 for Swedish, which is revisited in this study].

```{r}
#Write table of previous studies
norm_accounts_eval <- tibble(
      "Language investigated" = c(
        "US English",
        "US English",
        "US English",
        "US English",
        "US English",
        "US English, Norwegian, Swedish, German, Danish, Dutch",
        "UK English",
        "UK English",
        "Russian"),

      "Article" = c(
               "Barreda & Nearey, 2018",
               "Clopper, 2009",
               "Hindle, 1978",
               "Kohn & Farrington, 2012",
               "Labov, 2010",
               "Disner, 1980",
               "Fabricius, Watt & Johnson, 2009",
               "Flynn & Foulkes, 2011",
               "Lobanov, 1971"),

  "Speech materials" = c(
                         "120,000 simulated languages (of 5 or 9 vowels) modeled on Hillenbrand et al.'s (1995) data (98 female/male child/adult talkers * 12 vowels)",
                         "2 female/male talkers from Ohio (1 token * 10 vowels)",
                         "Peterson & Barney's (1952) database; 19 female/male talkers from Philadelphia + 60 telephone informants (minimum 3 tokens per category; analysis focus on /ay/)",
                         "Longitudinal data from 10 female/male African American talkers from North Carolina (approx. 10 tokens * 10 vowels * 5 ages)",
                         "Peterson & Barney's (1952) database; Philadelphia/Linguistic Change and Variation project (120 female/male talkers, stratified for age, sociolinguistic factors)",  
                         "Differing number of tokens, vowels, and phonetic contexts across the six languages",
                         "20 old/young female/male talkers of Received pronunciation (11 vowels); 6 old/young female/male talkers of Aberdeen English (8 vowels in different phonetic contexts)",
                         "20 old/young female/male Nottingham talkers (mean 180 recordings per talker; categories not reported)",
                         "5 female/male talkers (9 vowels in different phonetic contexts)"),

  "Normalization accounts" = c(
    "Nearey2, Lobanov, log-mean in linear regression framework",                                     
    "Bladon et al.'s scale factor of 1 Bark (1994), Syrdal & Gopal, Nordström & Lindblom, Nearey1, Nearey2, Watt & Fabricius, Gerstman, Lobanov, Miller",
    "Nearey2, Nordström-Lindblom, Sankoff-Shorrock-McKay",
    "Lobanov, Gerstman, Nearey1, Nordström & Lindblom, Syrdal & Gopal/Thomas, Watt & Fabricius",
    "Nearey2, Nordström-Lindblom, Sankoff-Shorrock-McKay ",
    "Gerstman, Lobanov, Nearey2, Harshman's PARAFAC model",
    "Watt & Fabricius, Lobanov, Nearey1",
    "log-transformation (base 10), log-transformation (natural), Mel, ERB, Bark (*2 gender-specific versions), Syrdal & Gopal, Nordström (*2 gender-specific versions), LCE, Gerstman, Lobanov, Watt & Fabricius (* 4 versions), lettER, Nearey (*4 versions)",
    "linear compression or expansion (Fant, 1960), Gerstman, Lobanov"),

  "Approach" = c(
    "distance between means (Eucledian distance)",                    
    "variance reduction (visual inspection)",
    "distance between means, variance reduction (regression)",
    "variance reduction (regression)",
    "distance between means (F-statistics)",
    "variance reduction (visual inspection)",
    "variance reduction (SCV in talker-means)",
    "variance reduction (SCV in talker-means)",
    "distance between means"),

  "Best two performing" = c(
    "log-mean in linear regression framework (1), Nearey2 (2)",
    "Nearey, Watt & Fabricius, Gerstman, Lobanov (no order)",
                            "Sankoff (1)",
                            "Lobanov (1), Gerstman, Watt & Fabricius (2)",
                            "Sankoff (1), Nearey2 (2)",
                            "Nearey2 (1), Lobanov (2)",
                            "Lobanov (1), Watt & Fabricius (2)",
                            "Gerstman (1), LCE (2)",
                            "Lobanov (1), Gerstman (2)"))
```

```{r norm-evaluation-variability, results='asis'}
norm_accounts_eval %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    caption = "Previous studies comparing the effectiveness of normalization accounts in reducing within-category cue variability.") %>%
  kable_styling(font_size = 7) %>%
  column_spec(1:2, width = "2cm") %>%
  column_spec(3, width = "5.5cm") %>%
  column_spec(4, width = "6cm") %>%
  column_spec(5:6, width = "2.5cm") %>%
  landscape() %>%
  collapse_rows()
```

\begin{landscape}\begin{table}

\caption{\label{tab:norm-evaluation-percep}Previous studies comparing normalization accounts in terms of their predicted consequences for perception.}
\centering
\fontsize{7}{9}\selectfont
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{2cm}>{\raggedright\arraybackslash}p{2cm}>{\raggedright\arraybackslash}p{5.5cm}>{\raggedright\arraybackslash}p{5.5cm}>{\raggedright\arraybackslash}p{2cm}>{\raggedright\arraybackslash}p{2cm}>{\raggedright\arraybackslash}p{1.5cm}}
\toprule
Language(s) investigated & Article & Speech materials & Normalization accounts & Approach & Accuracy assessed & Best two performing\\
\midrule
 & Barreda, 2021 & synthesized stimuli representing 6 talker types (based on data from 30 female/male talkers of California English (15 tokens * 11 vowels)) & Nearey2, Watt \& Fabricius, Lobanov & regression & against perceived category & Nearey2 (1), Watt \& Fabricius (2)\\
\cmidrule{2-7}
 & Carpenter \& Govindarajan, 1993 & Peterson \& Barney's (1952) database, 75 female/male child/adult talkers (2 tokens * 10 vowels) & Bark, Mel, ERB, 2 log-transformations, Syrdal \& Gopal, Miller, Nearey1, Nearey2, Gerstman, linear transformation (Watrous, 1993) & fuzzy ARTMAP, K-nearest neighbour & & linear transformation (1), Nearey1 (2)\\
\cmidrule{2-5}
\cmidrule{7-7}
 & Cole, Linebaugh, Munson \& McMurray, 2010 & 10 female/male talkers (3 tokens * 2 target vowels * 4 context vowels * 6 consonants) & C-CuRE & regression &  & C-CuRE (1)\\
\cmidrule{2-5}
\cmidrule{7-7}
 & Johnson \& Sjerps, 2021 & Peterson \& Barney's (1952) database, 75 female/male child/adult talkers (2 tokens * 10 vowels); Hillenbrand et al.'s (1995) database, 138 female/male child/adult talkers (1-3 tokens * 12 vowels) & Mean $\lambda$, F3 anchor, F1 anchor, Mean F* anchor (Sussman, 1986), Nordström, VTLN (Lammert \& Narayanan, 2015), Nearey2, Gerstman, VTLN ($\Delta$F), Nearey1, Watt \& Fabricius, Lobanov, Miller, Syrdal \& Gopal & support vector machine classification models &  & Lobanov (1), Watt \& Fabricius (2)\\
\cmidrule{2-5}
\cmidrule{7-7}
 & McMurray, Cole \& Munson, 2011 & Cole et al. (2010) database, 10 female/male talkers (1 token * 2 target vowels * 4 context vowels * 6 consonants) &  &  & \multirow{-12}{2cm}{\raggedright\arraybackslash against intended category} & \\
\cmidrule{2-3}
\cmidrule{6-6}
 & McMurray \& Jongman, 2016 & Jongman et al. (2000) database, 10 female/male talkers (1 token * 4 vowels * 8 fricatives) & \multirow{-2}{5cm}{\raggedright\arraybackslash C-CuRE} & \multirow{-2}{2cm}{\raggedright\arraybackslash regression} & & \multirow{-2}{2cm}{\raggedright\arraybackslash C-CuRE (1)}\\
\cmidrule{2-5}
\cmidrule{7-7}
 & Nearey, 1989 & synthesized stimuli of male child/adult talker (based on male talker data from Fant, 1973, and Peterson \& Barney, 1952) & intrinsic normalization, extrinsic normalization & response patterns (F-ratio) & & extrinsic effects (1), intrinsic effects (2)\\
\cmidrule{2-5}
\cmidrule{7-7}
 & Richter, Feldman, Salgado \& Jansen, 2017 & models based on Clopper \& Pisoni's (2006) NSP vowel corpus, 60 female/male talkers, 6 varieties (5 tokens * 10 vowels); perceptual data from Feldman et al., 2009 (synthesized stimuli of male talker) & Vocal Tract Length Normalization (VTLN), Lobanov & discrimination model likelihoods & \multirow{-6}{2cm}{\raggedright\arraybackslash against perceived category} & VTLN (1), Lobanov (2)\\
\cmidrule{2-7}
\multirow{-26}{2cm}{\raggedright\arraybackslash US English} & Syrdal, 1985 & Peterson \& Barney's (1952) database, 75 female/male child/adult talkers (2 tokens * 10 vowels) & log-transformation, Bark, Syrdal's bark-difference model, Miller (2 accounts), Nearey1, Nearey2, Gerstman & linear discriminant analysis &  & Nearey1 (1), Nearey2 (2)\\
\cmidrule{1-5}
\cmidrule{7-7}
Brazilian Portuguese \& US English & Escudero \& Hoffman Bion, 2007 & models trained on 400,000 F1-F2 combinations generated on recordings of 8 female/male talkers (20 tokens * 7 vowels and 15 tokens * 11 vowels) & Nearey1, Lobanov, Gerstman & constraint rankings &  & \\
\cmidrule{1-5}
Dutch & Adank, Smits \& van Hout, 2004 & 160 female/male talkers, 8 varieties (2 tokens * 9 vowels) & log-transformation, Bark, Mel, ERB, Syrdal \& Gopal, Lobanov, Nearey1, Nearey2\tablefootnote{Barreda \& Nearey (2018) identify a mistake in the implementation of the Nearey2 account in Adank et al. (2004), so that the relative performance of Nearey2 reported by Adank and colleagues should be interpreted with caution.}, Gerstman, Nordström, Miller & linear discriminant analysis & \multirow{-8}{2cm}{\raggedright\arraybackslash against intended category} & \multirow{-2}{2cm}{\raggedright\arraybackslash Lobanov (1), Nearey1 (2)}\\
\bottomrule
\end{tabular}
\end{table}
\end{landscape}

In the present study, we go beyond previous work by modeling the effects of normalization on the predicted perception of both vowel quality and vowel quantity over a particularly dense vowel space. Previous comparisons of normalization accounts have primarily focused on English [e.g., @adank2004; @barreda2018a; @carpenter1993; @clopper2009; @disner1980; @escudero2007; @fabricius2009; @Flynn2011; @hindle1978; @kohn2012a; @labov2010; @richter2017; @syrda1985]. Additional studies have investigated, for example, Dutch [@adank2004; @disner1980], Russian [@lobanov1971], and Brazilian Portuguese [@escudero2007]. The complexity of the vowel inventories (7-11 monophthongs) and the number of these vowels included in the comparison (2-11) varied across these studies. We add to this literature by comparing normalization accounts against a new phonetically annotated database of Central Swedish (SwehVd, introduced below). With a total of 21 monophthong allophones that vary in quantity (long vs. short vowels) and quality, the vowel inventory of Swedish is crowded compared to most languages previously studied in the normalization literature. This allows us to test whether the same normalization accounts that work well for simpler vowel inventories generalize well to more crowded vowel spaces.

To the best of our knowledge, only one previous study has compared normalization accounts against Swedish, as part of a cross-linguistic comparison across six Germanic languages [@disner1980]. @disner1980 compared 4 normalization accounts, using F1 and F2 means of the nine long Swedish vowels spoken by 24 male Swedish talkers [from a database presented in @fant1969]. Of interest to the present study, the results for Swedish differed from the other Germanic languages in two unexpected ways. Whereas Lobanov normalization---which involves centering and standardizing---performed best for Swedish, Nearey2 normalization---which involves only centering---performed best for the other four languages. And, while normalization effectively reduced inter-talker variability in category variances for the other four languages by 61%-71%, it was substantially less effective for Swedish (41%). As discussed by @disner1980, this raises the question as to whether these findings reflect an inherent property of Swedish or merely differences in the phonetically annotated databases available for each language. In particular, the Swedish data consisted of *vowels* produced in isolation without any lexical or phonetic context, whereas the data for the five other languages consisted of isolated *word* productions (paralleling the majority of research on normalization). The present study addresses this difference: the new database we introduce consists of *h*-VOWEL-*d* word recordings, which makes our stimuli directly comparable to those used in previous work on normalization, and lets us revisit whether simple *centering* accounts perform best for Swedish---like for the other languages in @disner1980. Additionally, we complement Disner's study by focusing on female, rather than male talkers, and by considering both long and short vowels (separately and together). The presence of quantity contrasts between long and short allophones makes Swedish a suitable case study to bridge the literature between vowel-specific normalization accounts (which focus on formants, and thus only quality contrasts) and general normalization accounts that can be applied to any type of cue (and thus also vowel duration, which is expected to be the primary cue to vowel quantity). While both F3 and vowel duration are known to be important cues to vowel categorization in Swedish [e.g., @behne1997; @fujimura1967; @haddingkoch1964], the two cues have never (duration) or rarely [F3, but see e.g. @adank2004; @barreda2018a; @carpenter1993; @nearey1989; @syrda1985] been included in comparisons of normalization accounts.

We compare the normalization accounts in Table \@ref(tab:norm-accounts) in terms of the predicted consequences for perception. The study compares accounts applied to (1) only F1 and F2, as in the majority of previous studies, (2) F1-F3, as in, e.g., @adank2004, and (3) F0-F3 as well as vowel duration. This allows us to assess whether differences in the effectiveness of normalization accounts depend on the number and types of cues that are considered. Since listeners integrate cues beyond F1 and F2 [e.g., @Assmann1982; @hillenbrand1999; @nearey1986], this is an important gap in evaluating the plausibility of different normalization accounts as models of adaptive speech perception. All three comparisons are evaluated both separately for short and long vowels, and for the entire space of the 21 vowels. This allows us to assess whether the same types of normalization perform well across the entire vowel inventory.

As shown in Table \@ref(tab:norm-evaluation-percep), previous work has employed a number of model types to compare the expected effects of normalization on perception, ranging from models based on phonological theory [e.g., optimality theory, @escudero2007], to more general models of categorization [e.g., linear discriminant analysis, @adank2004; @syrda1985; k-nearest neighbors as in exemplar theory or ARTMAP, @carpenter1993; Bayesian inference, @Kleinschmidt2018; @richter2017; support vector machine classification models, @johnson-sjerps2021], to general frameworks for data analysis [e.g., regression, @cole2010; @mcmurray-jongman2016]. In this study, we use a general model of speech perception, Bayesian ideal observers [e.g., @clayards2008; @nearey1986a; @norris-mcqueen2008], to predict the vowel identities in the SwehVd database under different normalization accounts. We then compare normalization accounts based on the recognition accuracy that they achieve when the (un)normalized cues are fed into the otherwise identical categorization model. We repeat this comparisons for different combinations of cues, and while categorizing different subsets of the vowel space. We use ideal observers, rather than other approaches, because *all* of their degrees of freedom can be estimated from the phonetic database we use [see also @tan2021; @xie2023]. In contrast, k-nearest neighbor categorization introduces the choice of a similarity metric, which can introduce one or more degrees of freedom into the modeling, and requires a choice for $k$. Similarly, linear discriminant analysis, support vector machines, or regression introduce *at least* one degree of freedom for each cue considered. This means that any comparison of normalization accounts needs to be conducted over the entire range of possible values for these degrees of freedom, making comparisons computationally more demanding and interpretation of the results more difficult.^[The consequences of additional degrees of freedom that we describe here hold for comparisons of normalization accounts against *production* data (as done here and in the majority of research on normalization). For the ultimate goal of comparing normalization accounts against data from human *perception*, additional degrees of freedom introduce additional questions. This includes questions about the *a priori* plausibility of an account as a theory of speech perception [for discussion, see @xie2023].] Bayesian ideal observers avoid this issue because of their assumption that listeners use and integrate cues *optimally*. As a consequence, the predicted posterior probabilities of all categories are fully determined by the combination of (1) the category-specific distribution of cues in the previous input and (2) the cue values of the input. The ideal observer approach employed here thus minimizes the degrees of freedom in the model that are not fully determined by the cue statistics in the input.

```{r load-SwehVd}
# Load native Swedish vowel data, manually corrected, but with distributional outliers remaining (generated by Persson-2021-outlierCorrected)
d.SwehVd <-
  read_csv('../../data/phonetic vowel statistics/Swedish/Persson_2021_L1_vowels_wDistrOutliers.csv')
```

All data and code for this article can be downloaded from OSF at <https://osf.io/zb8gx/>. This article is written in R markdown, allowing readers to replicate our analyses using freely available software [@R; @RStudio], while changing any of the parameters of our models. Readers can revisit and alter the assumptions we make---for example, categorization method, models of linguistic representations, the normalization accounts selected. The supplementary information (SI) lists the software/libraries required to compile this document.

# Methods {#sec:methodsII -}
We begin by introducing the new phonetically annotated corpus of Central Swedish vowel productions used in the present study. We then present the perceptual model that we use for assessing the predicted effects of different normalization accounts---a Bayesian ideal observer.

## Materials: The SwehVd database {#sec:swehvd -}
The SwehVd database is a new phonetically annotated corpus of Swedish *h*-VOWEL-*d* (short: hVd) word recordings. All recordings, annotations, and acoustic measurements are available on an OSF separate from the paper, at <https://osf.io/ruxnb/>. SwehVd was collected with the goal to characterize the Central Swedish vowel space within and across talkers---specifically, the regional standard variety of Swedish spoken in an area around and beyond Stockholm (eastern Svealand), including Mälardalssvenska, Sveamål, Uppsvenska, Mellansvenska [see e.g., @bruce2009; @elert1994; @riad2014].

SwehVd covers the entire monophthong inventory of Central Swedish, including all nine long vowels (*hid*, *hyd*, *hud*, *hed*, *häd*, *höd*, *had*, *håd*, *hod*), eight short vowels (*hidd*, *hydd*, *hudd*, *hedd*, *hädd*, *hödd*, *hadd*, *hådd*, *hodd*), and four allophones (*härd*, *härr*, *hörd*, *hörr*). To our knowledge, there are few publicly available databases of Swedish vowel productions that are phonetically annotated [e.g., @bruce1999; @eklund1997; @fant1969; @kuronen2000]. The largest and perhaps best-known is SweDia 2000 [@bruce1999]. SweDia 2000 was developed to characterize differences in vowel pronunciations *across* regional varieties of Swedish. It consists of recordings of spontaneous speech, isolated words in varying phonological contexts, and phrases in isolation from approximately 1300 talkers of 107 regional backgrounds, with 10-12 recorded talkers per region and 5-15 recordings per vowel for each talker.

Unlike most existing databases, SwehVd focuses on a single regional variety, providing high resolution within and across talkers for this variety: SwehVd consists of N=10 recordings of each hVd word (for a total of 210 recordings for the 21 different hVd words) per talker. Specifically, we target N = 24 male and female talkers each (current N = `r n_distinct(d.SwehVd$Talker)`, all female) for a total targeted N of tokens = 10,080 (current N = `r nrow(d.SwehVd %>% distinct(Talker, category, Token))` tokens). The database contains first to third formant (F1-F3) measurements for each talker at five time points across each vowel, together with vowel duration and mean F0 over the entire vowel.

SwehVd follows the gross of research on normalization and uses hVd words for recording in order to minimise coarticulatory effects from the surrounding phonetic context. The hVd context was originally chosen for studies on English because the glottal /h/ in onset position minimizes supraglottal articulations [confirmed in e.g., @chesworth2003; @robb2009]. Since then hVd words have played a central role in research on vowel production [e.g., @hillenbrand1995; @Peterson1952] and perception [e.g., @malinasky2020; @Peterson1952]. Since Swedish onset /h/ is a glottal approximant [@riad2014] similar to English, the use of this context in SwehVd facilitates comparison to similar databases from other languages. It deviates, however, from the majority of previous studies on Swedish vowels, which have either not held phonetic context constant across vowels [e.g., @bruce1999], or have investigated vowel production out of context [@eklund1997; @fant1969; @disner1980] or in different CVC contexts [e.g., *k*V*p* and *p*V*k* in @nordstrand2004; *v*V*t*, *v*V*tt*, *f*V*t*, *f*V*tt*, in @behne1997].

### The Swedish vowel inventory {#sec:sweVowelinventory -}
The Central Swedish vowel inventory contains 21 monophthong vowels. Seventeen of these vowels form nine pairs distinguished by quantity (long and short): in Central Swedish, the two long vowels [`r linguisticsdown::cond_cmpl("ɛː")`] and [`r linguisticsdown::cond_cmpl("eː")`] both neutralize to the same short vowel [`r linguisticsdown::cond_cmpl("ɛ")`] (resulting in a total of 17, rather than 18, distinct vowels). The two variants of a pair are considered allophones, the selection of which is determined primarily by stress and syllable complexity. Quantity is neutralized in unstressed positions [@riad2014].^[This reflects the mainstream analytical position in present-day Swedish phonology. The opposite position, distinctive vowel quantity, has also been proposed [e.g., @linell1978; @linell1979; @schaeffler2005a]. This theoretical debate does not affect the interpretation of our results.] Vowels lengthen in open word-final syllables, before morpheme-final single consonants, and in non-final syllables.

Additionally, there are four contextually conditioned allophones to [`r linguisticsdown::cond_cmpl("ɛ")`] and [`r linguisticsdown::cond_cmpl("ø")`]. Before /r/ (or any retroflex segment), both the long and short versions of these vowels lower to long and short [`r linguisticsdown::cond_cmpl("æ")`] and [`r linguisticsdown::cond_cmpl("œ")`], respectively. As shown in Table \@ref(tab:swedish-vowels) [adapted from @riad2014], some long-short vowel pairs are described to differ not only in quantity but also in quality: generally, short vowels are described as more open and also more centralized, forming a more condensed vowel space. In ongoing work [@persson2023], we found this to be confirmed for SwehVd.

Several of the long vowels have been claimed to be diphthongized in Central Swedish [e.g., @elert1981; @fant1969; @fant1971; @kuronen2000] and/or with consonantal elements [@mcallister1974], though empirical evaluations of this claim have returned mixed results [@eklund1997; @fant1969; @leinonen2010]. Here do not discuss this issue further [but see @persson2023] since it is unclear how the presence of diphthongization would *bias* our results (rather than to lead to worse performance across all accounts).

\begin{table}
\caption{\label{tab:swedish-vowels}The phonological characterization of long (left) and short (right) Central Swedish vowels (based on Riad, 2014)}
\centering
\begin{tabular}[t]{c|c|c|c|c}
\hline
&front&rounded&central&back\\
\hline
high&`r linguisticsdown::cond_cmpl("[iː]")`&`r linguisticsdown::cond_cmpl("[yː]")`&&`r linguisticsdown::cond_cmpl("[uː]")`\\
\hline
mid-high&`r linguisticsdown::cond_cmpl("[eː]")`&`r linguisticsdown::cond_cmpl("[ʉː]")`&&`r linguisticsdown::cond_cmpl("[oː]")`\\
\hline
mid&`r linguisticsdown::cond_cmpl("[ɛː]")`&`r linguisticsdown::cond_cmpl("[øː]")`&&\\
\hline
low&`r linguisticsdown::cond_cmpl("[æː]")`&`r linguisticsdown::cond_cmpl("[œː]")`&&`r linguisticsdown::cond_cmpl("[ɑː]")`\\
\end{tabular}
\hspace{2em}
\begin{tabular}[t]{c|c|c|c|c}
\hline
&front&rounded&central&back\\
\hline
high&`r linguisticsdown::cond_cmpl("[ɪ]")`&`r linguisticsdown::cond_cmpl("[ʏ]")`&`r linguisticsdown::cond_cmpl("[ɵ]")`&`r linguisticsdown::cond_cmpl("[ʊ]")`\\
\hline
mid&`r linguisticsdown::cond_cmpl("[ɛ]")`&`r linguisticsdown::cond_cmpl("[ø]")`&`r linguisticsdown::cond_cmpl("[œ]")`&`r linguisticsdown::cond_cmpl("[ɔ]")`\\
\hline
low&`r linguisticsdown::cond_cmpl("[æ]")`&&`r linguisticsdown::cond_cmpl("[a]")`&\\
\end{tabular}
\end{table}

### Participants {#sec:participants -}
Native talkers of Stockholm Swedish were recruited through word-of-mouth, flyers at Stockholm University Campus (see example flyer in SI (\@ref(sec:recruitment)), and online channels (accindi.se). Participants were selected based on the following criteria: L1 talkers of Swedish, born and raised in the greater Stockholm area or its surroundings, 20-40 years old (mean age=`r mean(d.SwehVd$Age) %>% round(digits = 0)`; SD=`r sd(d.SwehVd$Age) %>% round(digits = 2)`). All participants were reimbursed with a voucher to the value of SEK 100 after completing the recordings.

### Recording procedure {#sec:vowelCorpRec -}
Recording for the SwehVd database began in 2020 and is ongoing. The data were collected by the first author and Maryann Tan (Stockholm University). The hVd words were recorded together with another set of recordings targeting the production of Swedish word-initial stop voicing. Recording took place in a sound-attenuated room at the Multilingualism Laboratory, Department of Swedish Language and Multilingualism, Stockholm University.

Prior to recording, participants were informed about the study and given the possibility to ask questions before signing a consent form. They were then given instructions and seated at approximately 10 cm distance from an Audio Technica AT3035 microphone facing a computer screen. Words were presented one at a time, centered on screen, using PsychoPy software [@peirce2019]. Participants were instructed to read the words with their natural voice as they appeared on screen. Each talker read the same 21 target words, with 48 mono- and bi-syllabic filler words interspersed. Each target word was repeated 10 times and each filler word was repeated five times, generating a total of 450 productions per talker, 210 target productions and 240 filler productions. We generated two pseudo-randomized lists of the words, each list divided into four different blocks. Words were blocked across block lists and randomized within block lists, with the constraint that the same word would not appear more than twice in succession. Each participant was randomly assigned to one of the two lists. The pace of the presentation of the words was controlled by the experimenter, who was listening over Sennheiser HD215 headphones in the next room. A Yamaha MG102c mixing console with a built-in preamplifier was used together with a high-end ground isolator for preventing signal interference (Monacor FGA-40HQ). The speech was recorded at 44.1 kHz in Audacity [@teamaudacity2021]. Each long sound file was split into individual short sound files of one word each. The boundaries of each file were slightly trimmed and the files were labelled with the target word. All sound files from the same talker were concatenated into one long file before further processing.

The complete list of target hVd words is provided in Table \@ref(tab:word-list) in the SI. It consists of four real Swedish words, *hed*, *härd*, *hörd*, *hud* (English translations: *heath*, *hearth*, *heard*, and *skin*, respectively) and 17 phonotactically legal pseudowords. Following Swedish orthographical conventions for quantity, we used orthographic *hVdd* to elicit the short vowel allophone (e.g., *hudd* for [`r linguisticsdown::cond_cmpl("ɵ")`]) and orthographic *hVd* to elicit the long vowel allophone (e.g., *hud* for [`r linguisticsdown::cond_cmpl("ʉː")`]). This orthography reflects systematic phonological process of complementary quantity in Swedish [@riad2014]. In order to elicit the contextual allophones to [`r linguisticsdown::cond_cmpl("ɛ")`] and [`r linguisticsdown::cond_cmpl("ø")`], we added the supradental [`r linguisticsdown::cond_cmpl("ɖ")`] to elicit the long allophones (*härd*, *hörd*), and [`r linguisticsdown::cond_cmpl("r")`] to elicit the short allophones (*härr*, *hörr*). Challenges that came up during recording that were addressed are reported in the SI (\@ref(sec:challenges)).

The recordings were divided into five blocks: one practice block and four recording blocks, with breaks in between. The purpose of the practice block was threefold: to familiarize the participants with the recording procedure, to adjust the recording level, and if necessary, to further instruct the participant (e.g., if the participant used inappropriate or inconsistent intonation or stress pattern). Each recording block consisted of either 110 (N=2 blocks) or 120 (N=2 blocks) trials. The length of each block was approximately eight minutes, for a total of roughly 30 minutes recording time per talker. After the recording, participants filled out a language background questionnaire and received their reimbursement.

### Word and vowel segmentation {#sec:segmentation -}
SweFA, a Swedish version of the Montreal Forced Aligner developed by @young2021, was used to obtain estimates for word and segment boundaries. The boundaries were manually corrected by the first author (an L1 talker of Central Swedish). Following standard segmentation protocol and guidelines in @engstrand2001, segment boundaries were adjusted using spectrogram, waveforms and pitch and intensity tracks. The boundaries between /h/ and the vowel were adjusted to align with clear appearance of an F1, and the boundaries between the vowel and the coda consonant were aligned to a simultaneous rapid cessation of most or all formants.

### Extraction of phonetic cues {#sec:cue-extraction -}
We used the Burg algorithm in Praat [@boersma2022] to extract estimates of the first three formants (F1-F3) at five points of the vowel (20, 35, 50, 65, and 80 percent into the vowel). The following parameterization of the Burg algorithm was used:

 * Time step (s): 0.01
 * Max. number of formants: 5
 * Formant ceiling (Hz): 5500
 * Window length (s): 0.025
 * Pre-emphasis from (Hz): 50

In addition to F1-F3, we automatically extracted vowel duration and the fundamental frequency (F0) across the entire vowel. The Praat scripts that extract this information are shared as part of the SwehVd OSF repository, allowing researchers to choose additional or alternative time points at which to extract formants.

```{r set-outlier-cutoff-again, include=F}
# What proportion of the most extreme values should be considered outliers?
# (if set to e.g., .05 that means that points with cumulative densities below
# .025 or above .975 are considered outliers)
outlier_probability_cutoff = .01
```

```{r spectrogram, echo=FALSE, fig.cap="Example of Praat textgrid with annotated segment boundaries and measurement points for the automatic extraction of F1-F3 formant frequencies.", fig.width=base.width*1.5, fig.height=base.height*1.5, out.width='40%', fig.align='center'}
knitr::include_graphics("spectrogram.png")
```

In order to correct for measurement errors in the automatic extraction of cues, we estimated the joint multivariate distribution along all five extracted cues (F0, F1, F2, F3, and vowel duration) for each unique combination of vowel and talker. This approach allowed us to detect outliers relative to the joint distribution of the five cues for that vowel and talker. Points outside of the `r 100 * (outlier_probability_cutoff/2)`th to `r 100 * (1 - outlier_probability_cutoff/2)`th quantile of the multivariate Gaussian distribution of each vowel were identified, checked for measurements errors, and corrected. For measurements of the first three formants, we first checked the segmentation boundaries in the Praat textgrid and then manually measured new formant values using visual approximation of time points and Praat's function *Formant: Formant listing* or manually reading off the spectrogram. Segmentation boundaries were also checked for the identified vowel duration outliers. For measurements of F0, we extracted new estimated F0s across the vowel, after changing the pitch range settings. Given that there were still instances of pitch halving after measurement correction, in order to be conservative, we also checked all F0 values below the point of intersection between the two halves. This approach to F0 and formant correction strikes a middleground between the ideal (manual correction of all tokens) and feasibility. As SwehVd is open source, future work can contribute additional corrections to the database (e.g., via pull requests submitted to the repository linked on OSF). For the present purpose, additional undetected measurement errors are expected to bias *against* normalization, as outlier correction was conducted on the basis of raw F0 and formant values (Hz). If anything, the present study thus might under-estimate the effectiveness of normalization.

The procedure of adding written guides to *hod* and *hodd* to facilitate vowel identification was mostly successful, however not for all talkers. Some talkers corrected themselves after one trial, others failed to produce the intended vowel altogether. The SwehVd database contains columns for both the targeted vowel category, and the vowel category that the talker actually produced (as annotated by the first author).

```{r echo=FALSE}
# Keep code for reproducibility.
# d.SwehVd %<>%
#   mutate(Location = factor(Location))
# contrasts(d.SwehVd$Location) <- MASS::contr.sdif(5)
# m <- lmerTest::lmer(F2~ Location + (1 + Location | Talker), data = d.SwehVd %>% filter(category == "[iː]"))
# summary(m)
```

### Characterizing vowel productions in SwehVd {#sec:characterizing-swehvd -}

(ref:swe-vowels) The SwehVd vowel data in unnormalized F1-F2 space. Points show recordings of each of the 21 Central Swedish vowels by the 24 female native talkers in the database, averaged across the five measurement points within each vowel segment. Vowel labels indicate category means across talkers. Long vowels are boldfaced. Vowels that mismatched intended label are excluded (`r signif((d.SwehVd %>% filter(Transcribed_vowel != "targeted") %>% nrow() / d.SwehVd %>% nrow()) * 100)`% of all recordings). Note that the F1 and F2 axes are reversed. We follow this convention whenever plotting vowels in the F1-F2 space.

Figure \@ref(fig:swe-vowels) visualizes the vowel data from the SwehVd in F1-F2 space. The plot highlights the density of the Central Swedish vowel space, the categories are numerous and closely located. Category overlap is especially large among some of the high vowels (e.g., [`r linguisticsdown::cond_cmpl("iː")`] & [`r linguisticsdown::cond_cmpl("yː")`]; [`r linguisticsdown::cond_cmpl("uː")`], [`r linguisticsdown::cond_cmpl("oː")`]  & [`r linguisticsdown::cond_cmpl("ʊ")`]). The contextually conditioned allophone [`r linguisticsdown::cond_cmpl("æ")`], almost completely overlaps with the long [`r linguisticsdown::cond_cmpl("ɛː")`], whereas the contextual allophones to [`r linguisticsdown::cond_cmpl("ø")`] are more separated. Not all contextual allophones are articulated lower (higher F1) in relation to their phonemes [compare e.g., @riad2014]. They are, however, all articulated further back (lower F2). In line with Riad [-@riad2014, cf. Table \@ref(tab:swedish-vowels) above], the short vowels are overall more centralized and form a more condensed space, whereas the long vowels are more dispersed. We postpone a more in-depth characterization of the phonetic and phonological properties of the Central Swedish vowel system to ongoing work [@persson2023].

Figure \@ref(fig:swe-vowels-all-cues) visualizes the vowel data from the SwehVd database for all pairwise combinations of five cues: F0, F1, F2, F3 and vowel duration. As is to be expected, vowels differing in quality are most separated in the F1-F2 plot, indicating the two cues most important for vowel category distinction. However, the F1-F3 and F3-F2 plots both display less overlap between the high vowels [`r linguisticsdown::cond_cmpl("iː")`], [`r linguisticsdown::cond_cmpl("yː")`] and [`r linguisticsdown::cond_cmpl("ʉː")`], comparing to when plotted along F1-F2. The increased separation of these categories along F3 in vowel production data could point to the importance of F3 for some category distinctions, as found in previous studies [see e.g., @fant1969; @fujimura1967; @kuronen2000, for [`r linguisticsdown::cond_cmpl("iː")`] and [`r linguisticsdown::cond_cmpl("yː")`] categorization]. Also as expected, duration is the primary cue that distinguishes vowel quantity: in the last column of Figure \@ref(fig:swe-vowels-all-cues), the short vowels cluster on the left, and the long vowels on the right. They are separable, but overlapping. In addition to duration, F1-F3 can also carry information about vowels differing in quantity. This is evident, for example, for [`r linguisticsdown::cond_cmpl("iː")`] vs. [`r linguisticsdown::cond_cmpl("ɪ")`], [`r linguisticsdown::cond_cmpl("yː")`] vs. [`r linguisticsdown::cond_cmpl("ʏ")`], [`r linguisticsdown::cond_cmpl("ʉː")`] vs. [`r linguisticsdown::cond_cmpl("ɵ")`], [`r linguisticsdown::cond_cmpl("ɑː")`] vs. [`r linguisticsdown::cond_cmpl("a")`], [`r linguisticsdown::cond_cmpl("ɛː")`] vs. [`r linguisticsdown::cond_cmpl("ɛ")`] in F1-F2 space, and for [`r linguisticsdown::cond_cmpl("iː")`] vs. [`r linguisticsdown::cond_cmpl("ɪ")`], [`r linguisticsdown::cond_cmpl("yː")`] vs. [`r linguisticsdown::cond_cmpl("ʏ")`], [`r linguisticsdown::cond_cmpl("ʉː")`] vs. [`r linguisticsdown::cond_cmpl("ɵ")`] in F2-F3 space.

Finally, the densities along the diagonal of Figure \@ref(fig:swe-vowels-all-cues) suggest that F0 carries the least information about vowel identity, exhibiting the least between-category separation, followed by F3. This, too, is not surprising: while some accounts use F0 to _normalize_ F1 and F2 [e.g., @miller1989c; @Syrdal1986], F0 is not considered an important cue to vowel identity by itself [for demonstrations that F0 can, however, have strong *in*direct effects on vowel categorization, see @barreda2012a; @barreda2020a].

(ref:swe-vowels-all-cues) The same data as in Figure \ref{fig:swe-vowels} but for all pairwise combinations of five cues: F0, F1, F2, F3, and vowel duration. The primary purpose of this figure is to provide an overview of the SwehVd data. Additionally, comparisons across the panels sheds light on which cues carry information about vowel quality and vowel quantity, respectively. Note that, unlike in Figure \@ref(fig:swe-vowels), axis directions are not reversed. **Panels on diagonal:** marginal cue densities of all five cues. **Lower off-diagonal panels:** each point corresponds to a recording, averaged across the five measurement points within each vowel segment. Vowel labels indicate category means across talkers. Long vowels are boldfaced. **Upper off-diagonal panels:** Same data as in the lower off-diagonal panels but showing bivariate Gaussian 95% probability mass ellipses around category means. This makes it more obvious, for example, that long and short vowels are primarily distinguished by vowel duration (top right panel).

```{r swe-vowels, fig.width=base.width*3, fig.height=base.height*3, out.width='70%', fig.align='center', fig.cap="(ref:swe-vowels)"}
d.SwehVd %>%
  filter(Transcribed_vowel == "targeted", Word != "hädd") %>%
  # Get the geometric mean F1-F2 across the five time points
  group_by(Talker, category, Token, Quantity) %>%
  summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))) %>%
  ungroup() %>%
  ggplot(
    aes(
      x = F2,
      y = F1)) +
  geom_point(
    aes(
      colour = category,
      shape = Quantity),
    alpha = .4) +
  # Add between-talker mean
  geom_label(
    data =
      ~ .x %>%
      group_by(category, Quantity) %>%
      summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))),
    mapping = aes(
      colour = category,
      label = category,
      fontface = ifelse(category %in% levels.vowel.IPA.swe.long, 2, 1)),
    alpha = .8,
    size = 4,
    label.size = NA,
    label.padding = unit(.06, "cm")) +
  scale_colour_manual(name = "category", values = colors.vowel.swe) +
  scale_x_reverse("F2 (in Hz)", position = "top", breaks = scales::breaks_pretty(6)) +
  scale_y_reverse("F1 (in Hz)", position = "right", breaks = scales::breaks_pretty(6)) +
  guides(color = "none") +
  myGplot.defaults(base_size+2)  +
  theme(legend.position = "top")
```

```{r swe-vowels-all-cues, fig.width=base.width*3.5, fig.height=base.height*3.5+.5, out.width = '100%', fig.cap="(ref:swe-vowels-all-cues)"}
p.matrix.cues <-
  d.SwehVd %>%
  filter(Transcribed_vowel == "targeted", Word != "hädd") %>%
  # Get the geometric mean across all five time points for the five cues
  group_by(Talker, category, Token, Quantity) %>%
  summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))) %>%
  ungroup() %>%
  ggplot(
    aes(
      x = .panel_x,
      y = .panel_y)) +
  geom_point(
    aes(
      colour = category,
      shape = Quantity),
    alpha = .1) +
  geom_label(
    data =
      ~ .x %>%
      group_by(category, Quantity) %>%
      summarise(across(c(F0, F1, F2, F3, Duration), ~ mean(.x))),
    mapping = aes(
      colour = category,
      label = category,
      fontface = ifelse(category %in% levels.vowel.IPA.swe.long, 2, 1)),
    alpha = .4, size = 3, label.size = NA) +
  geom_autodensity(
    mapping = aes(fill = category, linetype = Quantity),
    alpha = .3, position = "identity") +
  stat_ellipse(
    mapping = aes(
      colour = category,
      linetype = Quantity),
    alpha = .4) +
  geom_point(
    data =
      ~ .x %>%
      group_by(category, Quantity) %>%
      summarise(across(c(F0, F1, F2, F3, Duration), ~ mean(.x))),
    mapping = aes(
      colour = category,
      shape = Quantity),
    alpha = .8, size = 1) +
  scale_colour_manual(name = "category", values = colors.vowel.swe, aesthetics = c("colour", "fill")) +
  guides(color = "none", fill = "none", shape = guide_legend(override.aes = list(alpha = .4))) +
  facet_matrix(
    vars(F0, F1, F2, F3, Duration),
    layer.lower = c(1,2),
    layer.diag = 3,
    layer.upper = c(4, 5)) +
  myGplot.defaults(base_size+2) +
  theme(legend.position = "top")

p.matrix.cues
```
```{r}
# Make temp exclusions file for identifying no of talkers to exclude
d.SwehVd.exclusions <-
  d.SwehVd %>%
  select(c(Talker, category, Transcribed_vowel, Token)) %>%
  distinct() %>%
  group_by(Talker, category) %>%
  summarise(
    N_tokens_per_category = n_distinct(Token),
    Transcribed_vowel = Transcribed_vowel) %>%
  filter(Transcribed_vowel != "targeted") %>%
  group_by(Talker, category, N_tokens_per_category) %>%
  summarise(
    N_unidentifiable_vowels = length(Transcribed_vowel)) %>%
  group_by(Talker, category) %>%
  mutate(
    exclude_Talker = case_when(N_tokens_per_category < 6 & N_unidentifiable_vowels >= 1 ~ "TRUE",
                               N_tokens_per_category >= 10 & N_unidentifiable_vowels > 6 ~ "TRUE",
                               TRUE ~ "FALSE")) %>%
  filter(exclude_Talker == TRUE)
```
```{r filter-data-for-study-create-CV}
set.seed(234215432)
# Exclude talkers that failed to produce *hodd* altogether + the mispronounced words by remaining talkers. This approach results in exclusion of an additional two talkers, given that they only did five recordings for each vowel. Also exclude measurements at 20 and 80% into the vowel, and all /hädd/ words since they are a duplicate of "[ɛ]" - the short allophone to /e/ neutralizes with the short allophone to /ɛ/. Also exclude all NAs in F0s.
d.SwehVd.forStudy <-
  d.SwehVd %>%
  filter(!Talker %in% d.SwehVd.exclusions$Talker, Transcribed_vowel == "targeted", Location %in% c(35, 50, 65), Word != "hädd", !is.na(F0)) %>%
  select(-Transcribed_vowel) %>%
  # Get the geometric mean across the three time points for the five cues
  group_by(Talker, Age, category, Word, Token, Trial, Quantity) %>%
  summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))) %>%
  ungroup()

# This creates K = 5 copies of the data, each distinguished by crossvalidation_group
# Each copy consists of K = 5 bins each, K - 1 = 4 of which are of fold_type "training"
# and 1 of which is of fold_type "test".
d.SwehVd.forStudy %<>%
# Split data into five equally sized bins  
  split_data() %>%
  #add groupID, assign bins to training or testing depending on groupID
  crossing(crossvalidation_group = 1:5) %>%
  mutate(
    fold_type = case_when(crossvalidation_group == 1 & fold != 5 ~ "training",
                     crossvalidation_group == 2 & fold != 1 ~ "training",
                     crossvalidation_group == 3 & fold != 2 ~ "training",
                     crossvalidation_group == 4 & fold != 3 ~ "training",
                     crossvalidation_group == 5 & fold != 4 ~ "training"),
    fold_type = replace_na(fold_type, "test")) %>%
  arrange(crossvalidation_group)
```

## Exclusions {-}
We use the SwehVd database with some exclusions. Since we are interested in assessing the effects of normalization, we excluded any productions on which the talker did not produce the targeted vowel. We then excluded all talkers (N = `r n_distinct(d.SwehVd.exclusions$Talker)`) with fewer than 5 remaining recordings for at least one of the vowels. This left data from `r n_distinct(d.SwehVd$Talker) - n_distinct(d.SwehVd.exclusions$Talker)` female native talkers, with on average `r d.SwehVd.forStudy %>% group_by(category) %>% tally() %>% summarise(x = mean(n)) %>% pull() %>% round(digits = 0)` (se= `r d.SwehVd.forStudy %>% group_by(category) %>% tally() %>% summarise(x = se(n)) %>% pull() %>% round(digits = 1)`) tokens per vowel (range = `r d.SwehVd.forStudy %>% group_by(category) %>% tally() %>% summarise(x = min(n)) %>% pull() %>% round(digits = 0)` to `r d.SwehVd.forStudy %>% group_by(category) %>% tally() %>% summarise(x = max(n)) %>% pull() %>% round(digits = 0)`), for a total of `r nrow(d.SwehVd.forStudy)` observations. We also exclude all *hädd* productions, as they elicited the same vowel as *hedd* (in line with @riad2014; see SI \@ref(sec:neutralize)). This way, we have about equally many tokens from all vowels, simplifying the cross-validation procedure presented below and facilitating visual comparisons across vowels in our figures.

Since our goal is to obtain a reliable estimate of the formant values during the steady state of the vowel, we use only the three formant measurements extracted from the middle of the vowel (at 35%, 50%, and 65% into the vowel).^[While this is the approach most commonly employed in the literature, it has the potential downside that co-articulation might affect formant values at the measurement points differently for long and short vowels (since the long and short vowels differ in overall duration). An alternative approach would be to extract formants at fixed durations (e.g., 30 ms) after the vowel onset and before the vowel offset. Since the findings we present below do not indicate any systematic differences in the performance of normalization accounts between long and short vowels, we do not consider this issue further here.]

## Modeling approach {-}
### Cues included in the normalization {#sec:cues -}
We compare the expected effects of different normalization accounts for the perception of Central Swedish vowels under three different assumptions about the relevant cues. The first comparison follows most previous research and focuses on the two primary cues to vowel perception, F1 and F2. The second comparison considers F3 in addition to F1 and F2, following @adank2004, @barreda2018a, @nearey1989 and @syrda1985.^[Some of these studies additionally included F0 [@adank2004; @nearey1989; @syrda1985]. However, since F0 is a cue that can display substantial cross-talker variability without directly contributing much information to vowel categorization (recall Figure \@ref(fig:swe-vowels-all-cues)), we decided to add only F3 to F1-F2 in the second evaluation.] Finally, the third comparison includes F0 and duration in addition to F1-F3. Since @Syrdal1986's bark-difference model only considers normalization along two dimensions---height, implemented as F1-F0, and backness, implemented as F2-F1---this account will only be included in the first comparison. Furthermore, given that C-CuRE is the only account that applies to any type of cue, we will consider duration as centered to each talker's mean (for the C-CuRE accounts), or as raw input (in ms; for all other accounts). We evaluate the predicted effects for perception both separately for long and short vowels, and on all 21 vowels together.

### Guarding against over-fitting: cross-validation {#sec:folds -}
As shown in Table \@ref(tab:norm-accounts), many of the normalization accounts involve parameters that are set based on the data [e.g., @gerstman1968; @lobanov1971; @mcmurray-jongman2011; @miller1989c; @nearey1978]. This raises the question of how much these parameters can be affected by outliers, or other issues such as over-fitting to the sample. Unlike previous work, we thus use 5-fold cross-validation to obtain 5 separate estimates of model predictions for each combination of normalization procedure and cues. Specifically, we randomly split the data for each unique combination of talker and vowel into 5 even parts (folds). On each of the five folds, we then fit the normalization parameters based on four of the folds (the training data) and evaluated the effects of the normalization on the fifth fold (the test data). This resulted in five model estimates for each combination of normalization procedure and cues. Our result graphs average over those folds.

```{r norm-data}
#Remove temp file
rm(d.SwehVd.exclusions)

# Normalize all data based on the respective training of each cross-validation group
d.SwehVd.forStudy %<>%
  group_by(crossvalidation_group) %>%
  # Get transformations, apply normalizations (both classic formants one and C-CuRE)
  group_map(
    .f = ~ apply_all_transformations_and_normalization(data = .x),
    .keep = T
  ) %>%
  reduce(bind_rows) %>%
  ungroup()

# Pivot data to long format for plotting and generation of IOs
d.SwehVd.forStudy.long <-
  d.SwehVd.forStudy %>%
  pivot_longer(
    cols = starts_with("F", ignore.case = FALSE),
    names_to = c("Cue", "Normalization.Scale", "Normalization.Type"),
    names_sep = "_",
    values_to = "Cue.Value") %>%
  pivot_wider(
    names_from = "Cue",
    values_from = "Cue.Value") %>%
  # Mutate both
  mutate(
    Normalization.Type = paste(Normalization.Scale, Normalization.Type, sep = "_")) %>%
  mutate(
    Normalization.Type = factor(plyr::mapvalues(Normalization.Type, levels.normalization, labels.normalization), levels = labels.normalization, ordered = T),
    Duration = ifelse(Normalization.Type %in% c("C-CuRE (Hz)", "C-CuRE (Mel)", "C-CuRE (ERB)", "C-CuRE (Bark)", "C-CuRE (semitones)"), Duration_CCuRE, Duration)) %>%
   select(-c(Duration_CCuRE, Normalization.Scale)) %>%
   relocate(Talker, Age, Normalization.Type, crossvalidation_group, fold_type, fold, category, Quantity, Token)
```

### Ideal Observers to predict the consequences of normalization for perception {#sec:IOs -}
Ideal observers provide an analytical framework for estimating how a rational listener would optimally behave in response to input (here: $n$-way alternative forced-choice categorization). Ideal observer models have been found to provide a good qualitative and quantitative fit against human speech perception [e.g., @clayards2008; @feldman2009; @kleinschmidt-jaeger2015; @kronrod2016; @norris-mcqueen2008; @xie2021cognition]. Unlike most other models of speech perception, ideal observers in their simplest form---as employed here---have zero degrees of freedom in the link from production to perception: once the ideal observer is trained on phonetic data from a database of *productions*, its predictions about *perception* are not mediated by additional parameters (unlike, e.g., exemplar models, connectionist accounts, or neural networks). 

In line with influential theories of speech perception [e.g., exemplar theory, @johnson1997; Bayesian accounts, @luce-pisoni1998; @nearey1990; @norris-mcqueen2008; interactive-activation accounts and their offsprings, @magnuson2020; @mcclelland-elman1986], ideal observers describe the posterior probability of a category as dependent both on the prior probability of the category in the current context, $p(category)$, and the likelihood of the acoustic input under the hypothesis that it originates from the category, $p(cues|category)$:

\begin{equation}
 p(category|cues) = \frac{p(cues|category) \times p(category)}{\sum_c p(cues|category_c) \times p(category_c)} (\#eq:Bayes-rule)
\end{equation}

The category prior, $p(category)$, describes how much the surrounding context favors each category. For the present study, the choice of category prior cannot affect the qualitative results since category priors are independent of the cues and held identical across all normalization accounts.^[Specifically, category priors have a constant additive effect on the posterior log-odds of categories.] We arbitrarily assume uniform category priors. Specifically, for ideal observers trained and tested on the long and short vowels separately, we model categorization as an 11- and 10-alternatives-forced-choice task, respectively, resulting in $p(category) = .091$ for the former and $p(category) = .1$ for the latter. For ideal observers trained and tested on the entire vowel space, we model categorization as a 21-alternatives-forced-choice task, resulting in $p(category) = .048$.

The likelihood, $p(cues|category)$, describes the distribution of cues for each category. Here, we follow previous work and assume multivariate Gaussian distributions to describe the cue likelihood [e.g., @clayards2008; @kleinschmidt-jaeger2015; @kronrod2016; @xie2021cognition]. That is, we use the model in equation \@ref(eq:Bayes-rule-normal), where $\mu$ and $\Sigma$ refer to the category mean and variance-covariance matrix of the category's multivariate normal distribution.^[Human perception is affected by an additional source of uncertainty beyond category variability: perceptual noise [for review, see @feldman2009]. Since the present study compares the *relative* recognition accuracy of different normalization accounts, it is not immediately obvious how the inclusion of noise could affect our results. To avoid additional researchers degrees of freedom---such as the decision as to which acoustic or perceptual space (Hz, Mel, Bark, etc.) perceptual noise is additive in---we do not model the perceptual consequences of noise.] In terms of representational complexity, the assumption of multivariate Gaussian categories strikes a compromise between exemplar storage [less representationally parsimonious, @johnson1997; @pierrehumbert2001] and cue integration over multiple separate univariate Gaussians [more parsimonious, @toscano-mcmurray2010; see also @xie2023]. Additionally, the multivariate approach entails optimal cue weighting, whereas optimal cue weights need to be determined separately for cue integration over independent univariate Gaussians.

\begin{equation}
 p(category|cues) = \frac{\mathcal{N}(cues| \mu, \Sigma) \times p(category)}{\sum_c \mathcal{N}(cues|\mu_c, \Sigma_c) \times p(category_c)} (\#eq:Bayes-rule-normal)
\end{equation}

Each ideal observer was trained on the training portion of the folded unnormalized and normalized data, and subsequently evaluated on the held-out test fold. This means that the parameters of each normalization account (e.g., the cue means in C-CuRE) and the resulting category parameters (the $\mu_c$s and $\Sigma_c$s for all categories) were set on the training data, and not changed for the test data. This reflects the realities of speech perception: although this is often ignored in evaluations of normalization accounts [e.g., @barreda2021; @mcmurray-jongman2011], listeners do not *a priori* know the cue means, cue variance, etc. of an unfamiliar talker. Rather, listeners need to incrementally *infer* those statistical properties from the talker's speech input [for discussion and a model, see @xie2023]. An additional advantage of cross-validation is that it gives us an estimate of the uncertainty about the model predictions. The performance of each ideal observer during test is assessed by calculating the ideal observer's predicted posterior probability of the *intended* category for each test token, under the accuracy-maximizing decision rule (criterion choice). 

```{r}
#Function for training IOs
train_io <- function(cues, data, group = NULL, no_noise = T, n.category = length(unique(data$category)), verbose = F) {
  cue_transformation = unique(gsub("^_([A-Za-z]+)_.*$" , "\\1", cues))
  cue_normalization = unique(gsub("^.*_([a-z])$" , "\\1", cues))

  # Add noise_variance depending on the no of cues
  noise_variance <- if(no_noise) {
    rep(0, n_distinct(cues))
  } else {
    rep(878, n_distinct(cues)) }# in Mel

  if (any(stringr::str_ends(cues, "_[cs]"))) {
    # calculate how much variance is changed compared to untransformed cues
    stats <-
      data %>%
      ungroup() %>%
      summarise_at(vars(contains("F")), sd) %>%
      select(cues, gsub("hertz", cues)) %>%
      pivot_longer(
        cols = everything(),
        names_pattern = "([A-Za-z]*)_(F[12])_([A-Za-z])",
        names_to = c("cue_transform", "formant", "cue_normalization")) %>%
      pivot_wider(names_from = "cue_normalization") %>%
      mutate(scale_factor = r / !! sym(cue_normalization))

    noise_variance <- noise_variance / stats$scale_factor
  }

  Sigma_noise <- diag(noise_variance)
  dimnames(Sigma_noise) <- list(cues, cues)

  m <- data %>%
    make_MVG_ideal_observer_from_data(
      verbose = verbose,
      category = "category",
      cues = cues,
      group = group,
      prior = rep(1/n.category, n.category),
      lapse_rate = 0,
      lapse_bias = rep(1/n.category, n.category),
      Sigma_noise = Sigma_noise
      )

  return(m)
}
```

```{r}
make_IOs_and_add_test_data <- function(data, cues, separate_by = NULL) {
  separate_by <- c(separate_by, "Normalization.Type", "crossvalidation_group")
  data %>%
  filter(fold_type == "training") %>%
  group_by(!!! syms(separate_by)) %>%
  # train IOs, one for each cross_validation group
  group_map(
    .f = ~ train_io(
      data = .x,
      cues = cues,
      no_noise = T) %>%
      nest(io = everything()) %>%
      bind_cols(.y)) %>%
    reduce(bind_rows) %>%
    # Add test data *that matches the IO*
    left_join(
      data %>%
        filter(fold_type == "test") %>%
        select(Talker, category, Token, !!! syms(cues), !!! syms(separate_by)) %>%
        # Renaming category here to avoid confusion downstream (with IO.Vowel)
        rename(Intended.Vowel = category),
      by = separate_by) %>%
    mutate(x = pmap(.l = list(!!! syms(cues)), ~ c(...))) %>%
    nest(test_data = c(Talker, Intended.Vowel, Token, !!! syms(cues), x)) %>%
    # Rename columns to identify each IO by
    mutate(
      Normalization.Type =
        factor(
          plyr::mapvalues(Normalization.Type, labels.normalization, levels.normalization.plots),
          levels = levels.normalization.plots)) %>%
    rename_with(
      .cols = separate_by,
      .fn = ~ paste0("IO.", .x)) %>%
    separate(IO.Normalization.Type, c("IO.cue_normalization", "IO.cue_transform"), remove = F) %>%
    mutate(
      IO.cue_transform = factor(IO.cue_transform, levels = c("Hz", "Mel", "Bark", "ERB", "log", "semitones")),
      IO.cue_normalization = factor(IO.cue_normalization, levels = c("r", "SyrdalGopal", "CCuRE", "Lobanov", "Nearey1", "Nearey2", "Gerstman", "Miller"))) %>%
    # Categorize test by IO
    mutate(
      # Store all of posterior distribution
      posterior_of_all_vowels = map2(
        test_data, io,
        ~ get_categorization_from_MVG_ideal_observer(
          x = .x$x,
          model = .y,
          decision_rule = "criterion",
          noise_treatment = "no_noise",
          lapse_treatment = "marginalize") %>%
          rename(Posterior = response, IO.Vowel = category) %>%
          # Join back in all other information about the test data
          left_join(.x) %>%
          relocate(Talker, Token, Intended.Vowel, observationID, x, everything())),
      # Store only the posterior of intended vowel
      posterior_of_intended_vowel = map(posterior_of_all_vowels, ~ .x %>% filter(Intended.Vowel == IO.Vowel)),
      # Format the posterior for all vowels to be wide
      posterior_of_all_vowels = map(posterior_of_all_vowels, ~ .x %>% pivot_wider(names_from = IO.Vowel, values_from = Posterior))) %>%
    relocate(starts_with("IO", ignore.case = F), io, test_data, posterior_of_all_vowels, posterior_of_intended_vowel)
}

# Make talker-independent IOs on long and short vowels *separately*, for different cue combinations
m.io.SwehVd.F1F2_by_quantity <-
  make_IOs_and_add_test_data(
    data = d.SwehVd.forStudy.long %>%
      #Filter down to the best performing implementation of Syrdal&Gopal
      filter(Normalization.Type != "SyrdalGopal2 (Bark)"),
    cues = c("F1", "F2"),
    separate_by = c("Quantity"))

m.io.SwehVd.F1F3_by_quantity <-
  make_IOs_and_add_test_data(
    data = d.SwehVd.forStudy.long %>%
      filter(!Normalization.Type %in% c("SyrdalGopal (Bark)", "SyrdalGopal2 (Bark)")),
    cues = c("F1", "F2", "F3"),
    separate_by = c("Quantity"))

m.io.SwehVd.all_by_quantity <-
  make_IOs_and_add_test_data(
    data = d.SwehVd.forStudy.long %>%
      filter(!Normalization.Type %in% c("SyrdalGopal (Bark)", "SyrdalGopal2 (Bark)")),
    cues = c("F0", "F1", "F2", "F3", "Duration"),
    separate_by = c("Quantity"))

# Make talker-independent IOs on long and short vowels *combined*, for different cue combinations
m.io.SwehVd.F1F2 <-
  make_IOs_and_add_test_data(
    data = d.SwehVd.forStudy.long %>%
      #Filter down to the best performing implementation of Syrdal&Gopal
      filter(Normalization.Type != "SyrdalGopal2 (Bark)"),
    cues = c("F1", "F2")) %>%
  # Add column for plotting
  mutate(
    IO.Quantity = "all")

m.io.SwehVd.F1F3 <-
  make_IOs_and_add_test_data(
    data = d.SwehVd.forStudy.long %>%
      filter(!Normalization.Type %in% c("SyrdalGopal (Bark)", "SyrdalGopal2 (Bark)")),
    cues = c("F1", "F2", "F3")) %>%
  # Add column for plotting
  mutate(
    IO.Quantity = "all")

m.io.SwehVd.all <-
  make_IOs_and_add_test_data(
    data = d.SwehVd.forStudy.long %>%
      filter(!Normalization.Type %in% c("SyrdalGopal (Bark)", "SyrdalGopal2 (Bark)")),
    cues = c("F0", "F1", "F2", "F3", "Duration")) %>%
  # Add column for plotting
  mutate(
    IO.Quantity = "all")

#Make combined dataframe for plotting
m.io.SwehVd <- rbind(
  m.io.SwehVd.F1F2_by_quantity %>%
    mutate(Cue_space = "F1-F2"),
  m.io.SwehVd.F1F2 %>%
    mutate(Cue_space = "F1-F2"),
  m.io.SwehVd.F1F3_by_quantity %>%
    mutate(Cue_space = "F1-F3"),
  m.io.SwehVd.F1F3  %>%
    mutate(Cue_space = "F1-F3"),
  m.io.SwehVd.all_by_quantity %>%
    mutate(Cue_space = "F0-F3, duration"),
  m.io.SwehVd.all %>%
    mutate(Cue_space = "F0-F3, duration")) %>%
  mutate(Cue_space = factor(Cue_space, levels = c("F1-F2", "F1-F3", "F0-F3, duration")))
```
```{r}
# Accuracies compared to the intended vowel
d.max.acc.toINTEND <- m.io.SwehVd %>%
  unnest(posterior_of_intended_vowel) %>%
  group_by(IO.Normalization.Type, Cue_space, IO.crossvalidation_group, IO.Quantity) %>%
  summarise(mean.acc = mean(Posterior)) %>%
  group_by(IO.Normalization.Type, Cue_space, IO.Quantity) %>%
  summarise(mean = mean(mean.acc))

d.max.acc.toINTEND.diff <- d.max.acc.toINTEND %>%
  group_by(Cue_space, IO.Quantity) %>%
  filter(mean == max(mean)) %>%
  left_join(d.max.acc.toINTEND %>%
              filter(IO.Normalization.Type == "r_Hz"),
            by = c("Cue_space", "IO.Quantity")) %>%
  mutate(diff = mean.x - mean.y) %>%
  ungroup()
```

# Results {-}
As an initial visualization of how normalization transforms the acoustic space, Figure \@ref(fig:swe-vowels-normalized) shows the transformed F1-F2 space for 5 of the accounts we evaluate. The SI (\@ref(sec:normVowelSpace)) provides plots of all 15 accounts.

(ref:swe-vowels-normalized) The 11 long vowels (**top**) and the 10 short vowels (**bottom**) of Central Swedish when F1 and F2 are left unnormalized or transformed into a perceptual scale (**grey**), intrinsically normalized (**yellow**), or extrinsically normalized through centering (**blue**) or standardizing (**purple**). Each point corresponds to one recording, averaged across the five measurement points within each vowel segment. Each panel combines the data from all five test folds.

```{r}
#Store grob.element
element_textbox_highlight <- function(
    ...,
    hi.labels = NULL, hi.fill = NULL,
    hi.col = NULL, hi.box.col = NULL,
    hi.labels2 = NULL, hi.fill2 = NULL,
    hi.col2 = NULL, hi.box.col2 = NULL,
    hi.labels3 = NULL, hi.fill3 = NULL,
    hi.col3 = NULL, hi.box.col3 = NULL
) {
  structure(
    c(ggtext::element_textbox(...),
      list(hi.labels = hi.labels, hi.fill = hi.fill, hi.col = hi.col, hi.box.col = hi.box.col,
           hi.labels2 = hi.labels2, hi.fill2 = hi.fill2, hi.col2 = hi.col2, hi.box.col2 = hi.box.col2,
           hi.labels3 = hi.labels3, hi.fill3 = hi.fill3, hi.col3 = hi.col3, hi.box.col3 = hi.box.col3)),
    class = c("element_textbox_highlight", "element_textbox", "element_text", "element",
              "element_textbox_highlight", "element_textbox", "element_text", "element",
              "element_textbox_highlight", "element_textbox", "element_text", "element"))
}

element_grob.element_textbox_highlight <- function(element, label = "", ...) {
  if (label %in% element$hi.labels) {
    element$fill <- element$hi.fill %||% element$fill
    element$colour <- element$hi.col %||% element$colour
    element$box.colour <- element$hi.box.col %||% element$box.colour
  }
  if (label %in% element$hi.labels2) {
    element$fill <- element$hi.fill2 %||% element$fill
    element$colour <- element$hi.col2 %||% element$colour
    element$box.colour <- element$hi.box.col2 %||% element$box.colour
  }
  if (label %in% element$hi.labels3) {
    element$fill <- element$hi.fill3 %||% element$fill
    element$colour <- element$hi.col3 %||% element$colour
    element$box.colour <- element$hi.box.col3 %||% element$box.colour
  }
  NextMethod()
}

# Plot vowel data in all normalization formats
p.vowels.norm <-
  d.SwehVd.forStudy.long %>%
  # filter down to test folds since that's what we later evaluate SI and IOs for + the best performing SyrdalGopal
  filter(fold_type == "test", Quantity == "long", Normalization.Type %in% c("transformed (Bark)", "Miller (log)", "C-CuRE (Hz)", "Nearey2 (log)", "Lobanov (Hz)")) %>%
  ggplot(
    aes(
      x = F2,
      y = F1)) +
  geom_point(
    aes(
      colour = category,
      shape = Quantity),
    alpha = 0.4,
    size = .7) +
  scale_colour_manual(name = "category", values = colors.vowel.swe) +
  scale_x_reverse("F2", position = "top", scales::pretty_breaks(n = 3)) +
  scale_y_reverse("F1", position = "right", scales::pretty_breaks(n = 3)) +
  guides(alpha = "none", color = "none", shape = "none") +
  facet_wrap(~ factor(Normalization.Type, levels = labels.normalization), scales = "free", ncol = 5) +
  theme(
    axis.title.y = element_blank()) +
    theme_half_open(12) +
    background_grid() +
    theme(
      strip.background = element_blank(),
      strip.text = element_textbox_highlight(
        size = 8,
        color = "black", fill = "#C9C0BB", box.color = "#C9C0BB",
        halign = 0.5, linetype = 1, r = unit(3, "pt"),
        #width = unit(1, "npc"),
        padding = margin(2, 3, 2, 3), margin = margin(3, 3, 3, 3),
        #margin = margin(0.6, 0.5, 0.5, 0.3),
        # this is new relative to element_textbox():
        # first named set
        hi.labels = c("SyrdalGopal (Bark)", "Miller (log)"),
        hi.fill = "#E6BE8A", hi.col = "black", hi.box.col = "#E6BE8A",
        # add second set
        hi.labels2 = c("C-CuRE (Hz)", "C-CuRE (Mel)", "C-CuRE (Bark)", "C-CuRE (ERB)", "C-CuRE (semitones)", "Nearey1 (log)", "Nearey2 (log)"),
        hi.fill2 = "#ABCDEF", hi.col2 = "black", hi.box.col2 = "#ABCDEF",
        # add third set
        hi.labels3 = c("Gerstman (Hz)", "Lobanov (Hz)"),
        hi.fill3 = "#DDADAF", hi.col3 = "black", hi.box.col3 = "#DDADAF"),
      axis.text.x = element_text(size=8, vjust=1),
      axis.text.y = element_text(size=8, hjust=1, vjust=.5),
      axis.title.x = element_text(size=8, vjust=0, hjust=0.5, face = "bold"),
      axis.title.y = element_text(size=8, hjust= 0.5, vjust=0.5, face = "bold"),
      legend.title = element_text(size=8, face = "bold", hjust= 0),
      legend.text = element_text(size=8),
      strip.placement = "outside",
      aspect.ratio = 1,
      panel.grid.major = element_blank())

p.vowels.norm.subset.short <- p.vowels.norm %+%
  (d.SwehVd.forStudy.long %>%
  # filter down to test folds since that's what we later evaluate SI and IOs for + the best performing SyrdalGopal
  filter(fold_type == "test", Quantity == "short", Normalization.Type %in% c("transformed (Bark)", "Miller (log)", "C-CuRE (Hz)", "Nearey2 (log)", "Lobanov (Hz)")))  +
  geom_point(aes(color = category),
             shape = 17,
             alpha = .4,
             size = .7)
```

\begin{landscape}

```{r swe-vowels-normalized, fig.width=base.width * 4, fig.height=base.height * 2.5, fig.align='center', out.width='90%', fig.cap="(ref:swe-vowels-normalized)"}
cowplot::plot_grid(p.vowels.norm, p.vowels.norm.subset.short, cols = 1)
```

\end{landscape}

Figure \@ref(fig:predictions) visualizes the unnormalized and normalized models' predictions for perception of Central Swedish vowels, under different assumptions about the relevant cues. This figure aggregates results across vowels of a given type (long, short, all). Additional studies in the SI (\@ref(sec:vowel-specific)) show results separately for each vowel, as well as visualizations summarizing how normalization affects the vowel-to-vowel confusion. These additional studies demonstrate, for example, that not all vowels benefit equally from normalization.

Averaging over all vowels, Figure \@ref(fig:predictions) highlights that the relative performance of the different normalization accounts within each panel is remarkably constant across panels. Regardless of the combination of cues or the vowel types considered (long, short, all), transformation into a perceptual space does little to improve recognition accuracy, compared to unnormalized cues. Intrinsic normalization, too, does not improve recognition accuracy. This replicates previous work on Dutch [@adank2004] but conflicts with some evaluations of English [e.g., @syrda1985]. @adank2004 discussed whether the discrepancy in results might be attributed to implementations of the Bark-transformation, or to what @syrda1985 describes as language-specificity of the second dimension of @Syrdal1986 normalization. The present results would seem to confirm this vulnerability of intrinsic normalizations. <!-- TO DO: Starting at @adank2004 discussed, this feels to me more like sth that should go into the discussion. Perhaps we do the same with any replication statements? (but at least these more complicated ones) -->
Extrinsic normalization, however, tends to substantially improve recognition accuracy (with the exception of Gerstman normalization). Depending on the specific combination of cues and the vowel qualities considered, the best-performing normalization model increases recognition accuracy by at least `r d.max.acc.toINTEND.diff %>% filter(mean.x == min(mean.x)) %>% pull(mean.x) %>% round(digits = 3) * 100`% (from `r d.max.acc.toINTEND.diff %>% filter(mean.x == min(mean.x)) %>% pull(mean.y) %>% round(digits = 3) * 100`% for unnormalized cues for all vowels when only F1-F2 are considered) to `r d.max.acc.toINTEND.diff %>% filter(mean.x == max(mean.x)) %>% pull(mean.x) %>% round(digits = 3) * 100`% (from `r d.max.acc.toINTEND.diff %>% filter(mean.x == max(mean.x)) %>% pull(mean.y) %>% round(digits = 3) * 100`% for short vowels when all cues are considered). The benefit of extrinsic normalization models, as well as the lower performance of perceptual transformations, replicates previous findings on other languages [e.g., @adank2004; @escudero2007; @nearey1989 found effects of both intrinsic and extrinsic accounts, but larger effects for extrinsic].

We also see that all models---even for unnormalized cues---perform substantially above chance. When long and short vowels are considered separately, the best ideal observers achieve recognition accuracies of `r d.max.acc.toINTEND.diff %>% filter(IO.Quantity == "long") %>% summarise(max = max(mean.x)) %>% round(digits = 3) * 100`% for long vowels and `r d.max.acc.toINTEND.diff %>% filter(IO.Quantity == "short") %>% summarise(max = max(mean.x)) %>% round(digits = 3) * 100`% for short vowels. For reference, in a recent perception experiment we conducted on the eight monophthongs of US English, L1-US English listeners achieved 71.1% accuracy in categorizing isolated hVd words [chance = 12.5%, @persson-jaeger2023]. A previous study on Swedish report an average recognition accuracy of 94.7% for the categorization of the long (isolated) vowels [@eklund1997]. The ideal observers for the Central Swedish vowel system thus achieve performance that is more or less comparable to that of human listeners, at least when cues are normalized.

Looking across columns of Figure \@ref(fig:predictions), short vowels are always recognized with higher accuracy compared to long vowels. This increase in performance cannot be explained by the small increase in the chance baseline alone (10% for the 10 short vowels, compared to 9.1% for the 11 long vowels). This result might initially be puzzling, given that previous descriptions of Central Swedish vowel inventories characterize the inventory of short vowels as being more centralized and more densely clustered [e.g., @kuronen2000; @riad2014]. Indeed, this claim seems to hold for SwehVd---compare Figures \@ref(fig:swe-vowels-normalized-long) and \@ref(fig:swe-vowels-normalized-short). However, they also exhibit less variability. Overall, this makes those vowels *easier* to recognize.

When long and short vowels are categorized together, performance of the ideal observers is comparatively poor unless vowel duration is included as a cue. This is expected given that vowel duration is the primary cue to vowel quantity. Of interest, however, is that even the inclusion of only F3 (second row) yields a substantial improvement in recognition accuracy, in line with @carpenter1993 [see also @hillenbrand1995 and @johnson-sjerps2021 for recognition accuracy of unnormalized F1-F2 vs. unnormalized F0-F3]. Remarkably, once vowel duration is included, the best-performing ideal observer achieves `r d.max.acc.toINTEND.diff %>% filter(IO.Quantity == "all", Cue_space == "F0-F3, duration") %>% summarise(max = max(mean.x)) %>% round(digits = 3) * 100`% recognition accuracy across the 21 long and short vowels (compared to chance = 4.8%).

Finally, looking across rows, we note that Lobanov normalization performs best especially when only the first two formants are considered. However, this advantage of Lobanov normalization decreases when additional cues are considered.^[\label{fn:best-performing} Indeed, when all five cues are considered for the categorization of all 21 short and long vowels, the best centering account perform numerically better (`r d.max.acc.toINTEND.diff %>% filter(IO.Quantity == "all", Cue_space == "F0-F3, duration") %>% summarise(max = max(mean.x)) %>% round(digits = 3) * 100`%) than Lobanov normalization (`r d.max.acc.toINTEND %>% filter(IO.Quantity == "all", Cue_space == "F0-F3, duration", IO.Normalization.Type == "Lobanov_Hz") %>% pull(mean) %>% round(digits = 3) * 100`%). This is, however, an artifact of our decision to only center vowel duration---the primary cue to vowel quantity---for the C-CuRE model. Separate modeling not shown here confirmed that Lobanov normalizaton achieves the same recognition accuracy as the C-CuRE models when duration is centered and combined with Lobanov-normalized formants (82.1%, 95%-CI: 79.2-84.9%).]

```{r}
# Plot the overall accuracy of the different normalization accounts
plot_io_results <- function(data) {
  data %>%
    # Rename IO.Normalization.Type for plotting
    mutate(
      IO.Normalization.Type =
        factor(
          plyr::mapvalues(IO.Normalization.Type, levels.normalization.plots, labels.normalization),
          levels = labels.normalization)) %>%
    unnest(posterior_of_intended_vowel) %>%
    # Get the CI over the five folds
    { if ("IO.Quantity" %in% names(data)) group_by(., IO.Normalization.Type, IO.Quantity, IO.crossvalidation_group, Cue_space) else  group_by(., IO.Normalization.Type, IO.crossvalidation_group, Cue_space) } %>%
    summarise(
      chance = 1 / n_distinct(Intended.Vowel, na.rm = TRUE),
      ci = list(enframe(Hmisc::smean.cl.boot(Posterior)))) %>%
    unnest(ci) %>%
    spread(name, value) %>%
    { if ("IO.Quantity" %in% names(data)) group_by(., IO.Normalization.Type, IO.Quantity, Cue_space) else  group_by(., IO.Normalization.Type, Cue_space) } %>%
    summarise(
      chance = first(chance),
      mean_CI_lower = mean(Lower),
      mean_CI_upper = mean(Upper),
      mean = mean(Mean)) %>%
    ggplot() +
    geom_pointrange(
      aes(x = IO.Normalization.Type,
          y = mean,
          ymin = mean_CI_lower,
          ymax = mean_CI_upper,
          color = IO.Normalization.Type),
      position = position_dodge2(width = 0.3),
      fatten = 2.5) +
    geom_text(mapping =
                 aes(x = IO.Normalization.Type,
                     y = mean_CI_upper + .04,
                     colour = IO.Normalization.Type,
                     label = sprintf("%0.2f", mean)),
               alpha = .85,
               size = 3,
               angle = 90,
               hjust = 0) +
    geom_hline(aes(yintercept = chance), color = "gray", linetype = 2) +
    scale_y_continuous("Overall accuracy (of predicting *intended* vowel)", limits = c(0,1))+
    scale_colour_manual(
      "Normalization \nprocedure of IO",
      labels = labels.normalization.1SG,
      values = colors.all.procedures.1SG) +
    guides(color = "none") +
    theme(axis.text.x = ggtext::element_markdown(angle = 60, vjust = 1, hjust = 1, colour = colors.all.procedures.1SG),
        axis.title.x = element_blank())
}

p.accuracy.io.overall <- plot_io_results(m.io.SwehVd) + facet_grid(Cue_space ~ factor(IO.Quantity, levels = c("long", "short", "all"), labels = c("long vowels", "short vowels", "all vowels")))
```

(ref:predictions) Predicted recognition accuracy of ideal observer under different normalization accounts for long vowels, short vowels, and all vowels together (columns), shown for for three different combinations of cues (rows). Labels indicate mean across the five test folds. Intervals show average bootstrapped 95% confidence intervals across the test folds. The dashed horizontal line indicates chance (different across columns because of the different number of long and short vowels).

```{r predictions, fig.width= base.width * 4.5, fig.height = base.height * 5.5, fig.align='center', out.width='95%', fig.cap="(ref:predictions)"}
p.accuracy.io.overall
```

# Discussion {#sec:discussion -}
We have compared low-level pre-linguistic normalization accounts against a new phonetically annotated database of Central Swedish vowels. We set out to evaluate how the different accounts differ in predicted consequences for perception. Previous work found that the types of normalization accounts that performed well on other languages did not seem to perform well on Swedish vowel data [@disner1980]. However, as pointed out by Disner, the Swedish data differed from the data for other languages in that study, and the majority of studies on other languages. Here, we followed the majority of previous work on vowel productions and analyzed productions of hVd recordings. We find that the same accounts found in previous work to perform well on other languages also perform well for the dense vowel space of Swedish. Specifically, Lobanov and centering approaches---incl. Nearey normalization and C-CuRE normalization---were the top-performing accounts, replicating the pattern found in previous studies on other languages [e.g., @adank2004; @carpenter1993; @escudero2007; @syrda1985]. This result suggests that the (somewhat) diverging results for Swedish in @disner1980's study, were not caused by properties inherent to Swedish, but more likely were an artifact of the dataset employed by Disner. It also suggests that languages with dense vowel spaces do not necessarily require more complex normalization mechanisms.

Evaluating the predicted effects of normalization against SwehVd has allowed for a comparison of how normalization accounts perform on subsets of a large vowel space and on the entire vowel space, while also evaluating the combined effects of different cues. By comparing performance on long and short vowels separately and together, we found that category variability seems to have a larger impact on model performance than the dispersion of the categories in the space. The highest model performance was achieved when models were trained on the short vowels that are more densely clustered but less variable, hence occupying a smaller perceptual space. Of importance for the evaluation of normalization is also that models patterned largely the same way across evaluations, indicating that the relative performance of each normalization account is the same regardless of the number of cues and size of vowel space. The best-performing centering accounts (C-CuRE) often achieve performance that is statistically indistinguishable from the best-performing standardization accounts (Lobanov). This is the case, in particular, when all five cues were considered and all 21 vowels were included in the categorization (see footnote \@ref(fn:best-performing)). Together with similar findings from research on consonants and supra-segmental categories [e.g., @apfelbaum2014; @crinnion2020; @kleinschmidt2019; @kulikov2022; @mcmurray-jongman2011; @mcmurray-jongman2016; @toscano2015; @xie2021cognition; @xie2023], this suggests that simple centering operations might be sufficient to maximize the benefits achievable by normalization. Given that these accounts involve computationally less complex operations, they might make up for a more plausible model of human perception, in contrast to standardizing accounts that involve more parameters for the listener to estimate. 

The inclusion of both long and short vowels in the present study also motivated the inclusion of a temporal cue alongside the spectral cues that have been the focus of previous studies. Overall, including duration improves the model accuracy across evaluations. More specifically, when all vowels are considered and duration is included as cue, we see the largest increase in model performance across models, with the best-performing accounts moving from `r d.max.acc.toINTEND %>% filter(Cue_space == "F1-F2", IO.Quantity == "all", IO.Normalization.Type == "CCuRE_Hz") %>% pull(mean) %>% round(digits = 3) * 100`% recognition accuracy when only F1-F2 are considered, to `r d.max.acc.toINTEND.diff %>% filter(Cue_space == "F0-F3, duration", IO.Quantity == "all") %>% pull(mean.x) %>% round(digits = 3) * 100`% when all cues are considered, and chance being as low as 4.8%. This confirms the importance of duration as acoustic-perceptual cue for vowel quantity distinctions. It furthermore suggests that temporal cues, such as duration, are susceptible to normalization, and that vowel normalization mechanisms operate not only in frequency domains but also time domains. General purpose accounts that can take any type of cue as input, such as C-CuRE, would presumably have an advantage against vowel-specific accounts, even more so in languages with a systematic quantity distinction, such as Swedish. Future studies could investigate the relative advantage of general purpose accounts for languages that does not have a systematic quantity distinction, to see whether the results generalize.

In the remainder of the discussion, we first summarize some methodological considerations based on the present study, and then discuss limitations of our work, and how they can be addressed in future work.

```{r}
#Remove temp file
rm(d.max.acc.toINTEND.diff)
```

## Methodological considerations {#sec:methods-disc -}
We used Bayesian ideal observers to compare normalization accounts. One advantage of this approach is that it reduces researchers' degrees of freedom, compared to other perceptual models. As already mentioned in the introduction, support vector machines  [@johnson-sjerps2021], k-nearest neighbors [@carpenter1993], or linear/logistic regression [@cole2010; @mcmurray-jongman2016] would necessarily introduce additional degrees of freedom in the link from production to predicted perception. We emphasize, however, that other researchers can download the R markdown document for this article (which contains the R code for our models) from OSF and substitute any other perceptual model for the ideal observers to assess the extent to which our choice of computational framework affects our findings.

The use of ideal observers also avoids the pitfalls of separability indices and other similar variability measures. In the SI (\@ref(#sec:auxiliaryStudy), we presents an auxiliary study  that compares normalization accounts in terms of a measure of category separability. Specifically, we compare the reduction of within-category variability relative to between-category variability (i.e., an F-statistics), facilitating comparison to the work summarized in Table \@ref(tab:norm-evaluation-variability) [e.g., @disner1980; @fabricius2009; @Flynn2011; @hindle1978; @labov2010]. This auxiliary study confirmed one particular advantage of ideal observers (and similar approaches) over measures of category variability/separability: by using the likelihood of an acoustic input under the multivariate distribution of all cues, ideal observers capture the *joint* effect of all cues. This captures that an input can be an improbable instance of a category based on one of its cue values but a probable instance given the values of all cues taken together. This is illustrated by Figure \@ref(fig:gaussians)A. In the SI, we show how a failure to account for such cases, can allow a single cue to dominate measures like the separability index, even if that cue is ultimately uninformative about category identity.

```{r}
set.seed(1423876)

set_cor <- function(m, cor) {
  m[1,2] <- sqrt(m[1,1]) * sqrt(m[2,2]) * cor
  m[2,1] <- m[1,2]
  return(m)
}

# Subset and manipulate dataset for visualization purposes
d.two_sim_vowels <-
  m.io.SwehVd.F1F2 %>%
  filter(
    IO.Normalization.Type == "r_Hz",
    IO.crossvalidation_group == 1) %>%
  select(io) %>%
  unnest(cols = c(io)) %>%
  select(category, mu, Sigma) %>%
  filter(category %in% c("[ɑː]", "[oː]")) %>%
  group_by(category) %>%
  mutate(
    F1 = map_dbl(mu, first),
    F2 = map_dbl(mu, last),
    Sigma = map2(Sigma, category, ~ if (.y == "[ɑː]") { set_cor(.x, .95) } else { .x }))

d.two_sim_vowels.samples <-
  d.two_sim_vowels %>%
  mutate(samples = pmap(list(200, mu, Sigma), rmvnorm)) %>%
  unnest(samples) %>%
  mutate(
    F1 = samples[,1],
    F2 = samples[,2],
    samples = NULL)

p.marginal.joint.1cat <-
  d.two_sim_vowels.samples %>%
    filter(category == "[ɑː]") %>%
  ggplot(
    aes(
      x = F2,
      y = F1,
      color = category)) +
  geom_point(alpha = .6, size = 1) +
  geom_path(
    data = get_bivariate_normal_ellipse(mu = d.two_sim_vowels %>%
          filter(category == "[ɑː]") %>%
          pull(mu) %>%
          .[[1]], Sigma = d.two_sim_vowels %>%
          filter(category == "[ɑː]") %>%
          pull(Sigma) %>%
          .[[1]]),
    aes(x = F2,
        y = F1),
    color = "#B3B3B3") +
  # annotate could be used here for all points
  geom_point(
    aes(
      x = quantile(F2, probs = .95),
      y = quantile(F1, probs = .05)),
    color = "black",
    size = 1) +
  geom_point(
    aes(
      x = quantile(F2, probs = .95),
      y = quantile(F1, probs = .95)),
    color = "black",
    size = 1) +
  geom_rug(
    mapping =
      aes(
        x = quantile(F2, probs = .95),
        y = quantile(F1, probs = .05)),
    color = "black",
    sides = "tr",
    length = unit(0.04, "npc"),
    linewidth = .9) +
  geom_rug(
    mapping =
      aes(
        x = quantile(F2, probs = .95),
        y = quantile(F1, probs = .95)),
    color = "black",
    sides = "tr",
    length = unit(0.04, "npc"),
    linewidth = .9) +
  annotate(
    "text",
    x=1225,
    y=340,
    label = "5\u1d57\u02b0 percentile",
    size = 3)  +
  annotate(
    "text",
    x=350,
    y=480,
    label = "5\u1d57\u02b0 \npercentile",
    size = 3)  +
  annotate(
    "text",
    x=350,
    y=820,
    label = "95\u1d57\u02b0 \npercentile",
    size = 3) +
  scale_color_manual(values = colors.vowel.swe) +
  scale_x_reverse(breaks = scales::pretty_breaks(n = 4), limits = c(1400, 200)) +
  scale_y_reverse(breaks = scales::pretty_breaks(n = 4), limits = c(900, 320)) +
  guides(color = "none")

# Marginal densities along x axis
xdens.A <-
  axis_canvas(p.marginal.joint.1cat, axis = "x") +
  stat_function(
    data = d.two_sim_vowels.samples %>%
      filter(category == "[ɑː]"),
    aes(x = F2,
    color = category),
    # theoretical normal density of *F2* along x-axis
    fun = function(x)
      dnorm(
         x,
        mean = d.two_sim_vowels %>%
          filter(category == "[ɑː]") %>%
          pull(mu) %>%
          .[[1]] %>%
          .[2],
        sd = sqrt(
          d.two_sim_vowels %>%
            filter(category == "[ɑː]") %>%
            pull(Sigma) %>%
            .[[1]] %>%
            .[2, 2])),
    color = "#B3B3B3") +
  scale_x_reverse()

# Marginal densities along y axis
# Need to set coord_flip = TRUE, if you plan to use coord_flip()
ydens.A <-
  axis_canvas(p.marginal.joint.1cat, axis = "y", coord_flip = TRUE) +
  stat_function(
    data = d.two_sim_vowels.samples %>%
      filter(category == "[ɑː]"),
    aes(x = F1,
        color = category),
    # theoretical normal density of *F1* along y-axis
    fun = function(x)
      dnorm(
        x,
        mean = d.two_sim_vowels %>%
          filter(category == "[ɑː]") %>%
          pull(mu) %>%
          .[[1]] %>%
          .[1],
        sd = sqrt(
          d.two_sim_vowels %>%
            filter(category == "[ɑː]") %>%
            pull(Sigma) %>%
            .[[1]] %>%
            .[1, 1]))) +
  scale_color_manual(values = colors.vowel.swe) +
  coord_flip() +
  scale_x_reverse(breaks = scales::pretty_breaks(n = 4))

p.marginal.joint.1cat <-
  insert_xaxis_grob(
    p.marginal.joint.1cat,
    xdens.A,
    grid::unit(.2, "null"),
    position = "top")

p.marginal.joint.1cat <-
  insert_yaxis_grob(
    p.marginal.joint.1cat, ydens.A,
    grid::unit(.2, "null"),
    position = "right")

p.marginal.joint.1cat <- ggdraw(p.marginal.joint.1cat)
```

```{r}
p.marginal.joint.2cat <-   d.two_sim_vowels.samples %>%
  ggplot(
    aes(
      x = F2,
      y = F1,
      color = category)) +
  geom_point(alpha = .6, size = 1) +
  geom_path(
    data = get_bivariate_normal_ellipse(
      mu = d.two_sim_vowels %>%
            filter(category == "[ɑː]") %>%
            pull(mu) %>%
            .[[1]],
      Sigma = d.two_sim_vowels %>%
            filter(category == "[ɑː]") %>%
            pull(Sigma) %>%
            .[[1]]),
    aes(x = F2,
        y = F1),
    color = "#B3B3B3") +
  geom_path(
    data = get_bivariate_normal_ellipse(
      mu = d.two_sim_vowels %>%
            filter(category == "[oː]") %>%
            pull(mu) %>%
            .[[1]],
      Sigma = d.two_sim_vowels %>%
            filter(category == "[oː]") %>%
            pull(Sigma) %>%
            .[[1]]),
    aes(x = F2,
        y = F1),
    color = "#A6CEE3") +
  geom_point(
    data = get_bivariate_normal_ellipse(
      mu = d.two_sim_vowels %>%
                filter(category == "[ɑː]") %>%
                pull(mu) %>%
                .[[1]],
      Sigma = d.two_sim_vowels %>%
                filter(category == "[ɑː]") %>%
                pull(Sigma) %>%
                .[[1]],
      level = .9,
      segments = 4) %>%
      filter(F1 %in% range(F1)),
    aes(
      x = F2,
      y = F1),
    color = "black",
    size = 1) +
  scale_color_manual(values = colors.vowel.swe) +
  scale_x_reverse(breaks = scales::pretty_breaks(n = 4), limits = c(1400, 200)) +
  scale_y_reverse(breaks = scales::pretty_breaks(n = 4), limits = c(900, 320)) +
  guides(color = "none")

# Marginal densities along x axis
xdens.A <-
  axis_canvas(p.marginal.joint.2cat, axis = "x") +
  stat_function(
    data = d.two_sim_vowels.samples,
    aes(x = F2),
    # theoretical normal density of *F2* along x-axis
    fun = function(x)
      dnorm(
         x,
        mean = d.two_sim_vowels %>%
          filter(category == "[ɑː]") %>%
          pull(mu) %>%
          .[[1]] %>%
          .[2],
        sd = sqrt(
          d.two_sim_vowels %>%
          filter(category == "[ɑː]") %>%
            pull(Sigma) %>%
            .[[1]] %>%
            .[2, 2])),
    color = "#B3B3B3") +
  #Get density for the second category
  stat_function(
    data = d.two_sim_vowels.samples,
    aes(x = F2),
    # theoretical normal density of *F2* along x-axis
    fun = function(x)
      dnorm(
         x,
        mean = d.two_sim_vowels %>%
          filter(category == "[oː]") %>%
          pull(mu) %>%
          .[[1]] %>%
          .[2],
        sd = sqrt(
          d.two_sim_vowels %>%
          filter(category == "[oː]") %>%
            pull(Sigma) %>%
            .[[1]] %>%
            .[2, 2])),
    color = "#A6CEE3") +
  scale_x_reverse()

# Marginal densities along y axis
ydens.A <-
  axis_canvas(p.marginal.joint.1cat, axis = "y", coord_flip = TRUE) +
  stat_function(
    data = d.two_sim_vowels.samples,
    aes(x = F1),
    # theoretical normal density of *F1* along y-axis
    fun = function(x)
      dnorm(
        x,
        mean = d.two_sim_vowels %>%
          filter(category == "[ɑː]") %>%
          pull(mu) %>%
          .[[1]] %>%
          .[1],
        sd = sqrt(
          d.two_sim_vowels %>%
          filter(category == "[ɑː]") %>%
            pull(Sigma) %>%
            .[[1]] %>%
            .[1, 1])),
    color = "#B3B3B3") +
  #Add density for second category
  stat_function(
    data = d.two_sim_vowels.samples,
    aes(x = F1),
    # theoretical normal density of *F1* along x-axis
    fun = function(x)
      dnorm(
         x,
        mean = d.two_sim_vowels %>%
          filter(category == "[oː]") %>%
          pull(mu) %>%
          .[[1]] %>%
          .[1],
        sd = sqrt(
          d.two_sim_vowels %>%
          filter(category == "[oː]") %>%
            pull(Sigma) %>%
            .[[1]] %>%
            .[1, 1])),
    color = "#A6CEE3") +
  coord_flip() +
  scale_x_reverse()

p.marginal.joint.2cat <-
  insert_xaxis_grob(
    p.marginal.joint.2cat,
    xdens.A,
    grid::unit(.2, "null"),
    position = "top")

p.marginal.joint.2cat <-
  insert_yaxis_grob(
    p.marginal.joint.2cat,
    ydens.A,
    grid::unit(.2, "null"),
    position = "right")

p.marginal.joint.2cat <- ggdraw(p.marginal.joint.2cat)
```

(ref:gaussians) Using a perceptual model to evaluate normalization accounts avoids the pitfalls of separability/variability indices. **Panel A:** an acoustic token can be an improbable instance of a category if each cue is considered separately (the marginal densities along the sides of the plot), but highly probable if considered relative to the joint distribution of cues (the bivariate distribution indicated by the ellipse). **Panel B:** two acoustic tokens that are equally far from the mean of one category can have radically different consequences for perception, depending on where the tokens fall relative to other categories. Under the hypothesis that the two black points are instances of the gray category, they would be attributed the same separability index but radically different probabilities given the joint distribution of cues relative to the other category in the space. Both points are on the 90% highest density interval isoline.

```{r gaussians, fig.width= base.width * 3.2, fig.height = base.height * 1.25+.5, fig.align='center', out.width='90%', fig.cap="(ref:gaussians)"}
plot_grid(p.marginal.joint.1cat, p.marginal.joint.2cat, nrow = 1, labels = c("A", "B"))
```

Second, by normalizing the support for a category by the support for all other categories (the denominator in Equations \@ref(eq:Bayes-rule) and \@ref(eq:Bayes-rule-normal)), ideal observers consider the perceptual consequences of an acoustic input *relative to all possible categories*. This means that a token that is relatively far away from its category mean does not necessarily result in low recognition accuracy. Rather, low recognition accuracy is only predicted if the relative position of the acoustic input in the acoustic-phonetic space makes it more probable that the input originated from another (unintended) category. This is illustrated in Figure \@ref(fig:gaussians)B, and parallels human perception: e.g., while a more mid-fronted `r linguisticsdown::cond_cmpl("[ɑː]")` with high F1- and F2-values is atypical, human listeners are more likely to recognize it as a `r linguisticsdown::cond_cmpl("[ɑː]")` compared to a more high-back articulated, but equally atypical, `r linguisticsdown::cond_cmpl("[ɑː]")`, presumably because the observed phonetics would be equally likely to occur if the talker intended a `r linguisticsdown::cond_cmpl("[oː]")`. Measures of between- vs. within-category variability like the separability index in the auxiliary study, however, have no means of directly capturing this. On the contrary, the use of *squared* distances in the separability index means that even a few observations located far away from the category mean can disproportionally affect the index, regardless of how probable these tokens would be. If a category under some normalization account have non-zero cue densities far away from the mean, they will be assigned low category separability even though observations with such cue values are rare. In the same way, the use of squared distances can result in high category separability even if a cue separates only a small subset of categories, compared to another cue that more gradiently separates *all* categories. Unlike a separability index, however, the ideal observer will not disproportionally weight a single observation: it will simply be one of many observations---much like human speech perception does not collapse simply because one word (or one vowel) has been misunderstood. It will thus correctly predict low recognition accuracy when an acoustic input is indeed improbable under its intended category relative to other unintended categories.

Finally, measures like the separability index suffer from a conceptual issue: the goal of speech perception is presumably not to reduce cue variability around the category mean but rather to increase the probability of correct recognition (of both linguistic and social information, where we focus on the former here). These two goals are not the same [see also discussion in @barreda2020a]. In sum, indices of variability and category separability like the one applied in the SI (see Equation \@ref(eq:separability-index) in [SI, Separability index](#sec:separabilityIndex)) fail to adequately assess the expected consequences of normalization for perception, which is the primary interest of this paper, and addressed by the adopted methodology.

<!--AP: copied into introduction, as suggested by R2. Kept here for now::: While these different approaches will generally return similar results, we see two advantages with ideal observers. First, ideal observers remove researchers' degrees of freedom in the evaluation of normalization accounts [see @tan2021]: the statistics of the cue distributions in the training data fully determine the ideal observers' predictions. Second, representing the cue likelihood for each category as multivariate Gaussian distributions [e.g., @kronrod2016; @norris-mcqueen2008; @xie2021cognition], strikes a middle ground between less parsimonious models such as exemplar models [e.g., @johnson1997; @pierrehumbert2001], and more parsimonious models, such as models of cue integration over multiple independent univariate Gaussians [e.g., @toscano-mcmurray2010].^[Parsimony here refers both to the number of degrees of freedom these models afford to the researcher *and* to the amount of information that listeners are assumed to store [for discussion, see @xie2023].]-->

As a final note on methodology, we find the 5-fold cross-validation approach used for normalization of the acoustic data adopted here, advantageous for several reasons, the two most important being that: (1) it allows us to avoid over-fitting to the sample, while also (2) providing a more realistic reflection of how parameters used for normalization are incrementally inferred from the talker's speech input. Even though many of the commonly adopted normalization accounts involve parameters that are set based on the data, previous studies have rarely considered how these parameters might be affected by the specific dataset used for normalization [for exceptions, see e.g., @barreda2018a]. In the present study, we have seen that accounting for researchers' uncertainty about the effects of normalization, highlights that many of the normalization accounts exhibit statistically indistinguishable performance---at least under the approach taken here and in the majority of previous work. This means that future studies should aim to increase statistical power, in order to determine the normalization mechanism that best describes human behavior. This will require even larger datasets, and/or by targeted sampling of vowel tokens for which predictions of different normalization accounts are maximally contrasted [see e.g., @barreda2021; and see @xie2023 for more general discussion of how to increase the statistical power to determine what mechanism underlie adaptive speech perception].

```{r}
#Remove fake data for plot generation
rm(d.two_sim_vowels, d.two_sim_vowels.samples, d.max.acc.toINTEND)
```

## Limitations and future directions {#sec:limitations -}
Four limitations of the present study, three of which are shared with most previous work, deserve discussion. First, the present study compared normalization accounts against speech from only female talkers of one regional variety of Central Swedish (Stockholm Swedish). In contrast, many previous studies included data from talkers of different genders [e.g., @barreda2021; @clopper2009; @cole2010; @mcmurray2011; @mcmurray-jongman2016], and sometimes from talkers of different ages [e.g., @barreda2018a; @carpenter1993; @Flynn2011; @hindle1978; @johnson-sjerps2021; @kohn2012a; @syrda1985] and/or language backgrounds [e.g., @adank2004; @disner1980; @escudero2007; @fabricius2009; @labov2010; @richter2017]. Given that age, gender, etc. tend to affect formants (and other cues) beyond talker-variability, it is likely that the inclusion of more diverse talkers would increase the lack of invariance problem. For example, we would expect the ideal observers over unnormalized cues to achieve lower recognition performance if vowel productions from male talkers would be included in the data. In short, the models likely over-estimates the recognition accuracy that can be achieved for unnormalized cues if a more diverse range of talkers is considered.

What does this imply for our conclusions about the relative effect of normalization? To the extent that normalization successfully overcomes inter-talker variability that is caused by gender, age, and other social or physiological factors, we expect that the benefit of normalization accounts should show more clearly, relative to unnormalized cues. In this sense, the present study might *under*-estimate the relative benefits of normalization. Whether the *relative* performance of normalization accounts---i.e., the finding of primary interest to us---would differ if a more diverse range of talkers was considered is unclear. To the extent that vowel-specific accounts were originally developed specifically to eliminate physiological differences that are correlated with gender [as reviewed in, e.g., @johnson-sjerps2021], it is theoretically possible that the high performance of general normalization accounts [e.g., C-CuRE, @mcmurray-jongman2011] might not replicate when talkers of different genders are included. Future releases of the SwehVd database will contain data from male talkers, which will allow us or other researchers to revisit these questions.

Second, the present study aggregated acoustic-phonetic measurements taken at different points of the vowel (at 35%, 50%, and 65% into the vowel) into a single formant measurement. This follows previous comparisons of normalization accounts but is a simplifying assumption that should be revisited in future work. Formant dynamics carry important information for category distinctions [e.g., @Assmann2005; @hillenbrand1999; @nearey1986], and are hypothesized to be of particular importance for some vowel distinctions in other varieties of Central Swedish [e.g., @kuronen2000]. Prior to other consideration, this means that this study likely under-estimates the recognition accuracy that could be achieved even from unnormalized cues alone. It is an open question whether the findings of primary interest---the relative performance of different normalization accounts---would be affected if formant dynamics were considered. Some normalization accounts, for example, consider normalization of such formant dynamics to take place *after* basic formant normalization (but before the mapping of cues to category representations, S. Barreda, personal communication, 01/06/2023).<!-- TO DO: or perhaps this is even in one of the papers? --> Future work could employ SwehVd to compare ideal observers or other classification models while taking into consideration formant measurements throughout the vowel.

Third, we only considered competing *normalization* accounts. This, too, follows previous research on normalization but is potentially problematic. As mentioned in the introduction, it is now believed that at least three different mechanisms contribute to adaptive speech perception, including not only normalization but also changes in category representations and decision-making [for review, see @xie2023]. This has consequences for research on normalization. For example, @xie2021cognition compared normalization accounts against the production of prosodic phrasing in L1-US English, while also considering alternative hypotheses about listeners' ability to adapt category representations. Xie and colleagues found that the effectiveness of cue normalization is substantially reduced if listeners can learn and maintain talker- or group-specific category representations [as assumed in some influential theories of speech perception, exemplar models, e.g., @johnson1997; @pierrehumbert2001; Bayesian ideal adaptors, @kleinschmidt-jaeger2015]. Xie and colleagues only considered two general types of normalization, and their focus was on the interpretation of prosodic signals. But their results call for caution in interpreting studies like the present that do not consider the possibility of talker-specific representations---an assumption shared with basically all previous work on vowel normalization.

Similarly, as mentioned in the introduction, we limited our evaluation to a single level of normalization (and combinations of perceptual transformations and a single level of normalization). Some proposals, however, assume multiple separate normalization steps. For example, some accounts hold that evolutionarily early mechanisms first transform spectral percepts into a phonetic space [e.g., uniform scaling accounts, @barreda2020a; @nearey1983], on which additional subsequent normalization might operate. There is also evidence that speech perception combines aspects of intrinsic and extrinsic normalization [@johnson-sjerps2021 review relevant evidence from brain imaging; early behavioral evidence is found in @nearey1989]. The present study---like most existing evaluations---did not consider these possibilities [for exceptions, see e.g., @barreda2021; @nearey2007].

Fourth and finally, we followed the majority of previous work and evaluated normalization accounts against *production* data. This is potentially problematic, especially when measures like category separability or reduced cross-talker variability in category means are used to evaluated normalization accounts (as in the auxiliary study in the SI and in many previous studies). These evaluations essentially assume that the goal of speech perception is to make the perceptual realizations of the same category by different talkers as similar as possible in the normalized space [for an in-depth critique, see @barreda2021]. However, the goal of speech perception is presumably to reliably infer the category intended by the talker,^[Or somewhat more precisely, cooperative listeners aim to understand the meaning intended by the talker, and this inference is generally believed to benefit from the correct recognition of phonological categories, such as phonemes, syllables, or word forms. For discussion, see also @hume2016.] and this aim does not necessarily entail perfect removal of cross-talker variability (as evidenced, for example, by the different findings of the main study and the auxiliary study in the SI).

To some extent, our study addresses this potential issue by evaluating normalization accounts in terms of how well they predict the vowel category intended by the talker. However, if the goal is to explain human perception, the most informative evaluations of normalization accounts are arguably those that compare their predictions against *listeners'* behavior [for examples, see @barreda2020a; @barreda2021; @mcmurray-jongman2016; @nearey1989; @richter2017; @xie2021cognition]. In short, approaches like that employed here take an important step away from the most misleading evaluation of normalization accounts in terms of reduced category variability/increased category separability. Ultimately, however, normalization accounts should be evaluated in terms of how well they predict listeners' perception, not talker's intention.

# Disclosure/Conflict-of-Interest Statement {-}
The authors declare that the research was conducted in the absence of any
commercial or financial relationships that could be construed as a potential
conflict of interest.

# Author Contributions {-}
AP proposed project idea, and both authors jointly developed the conceptual approach. AP designed SwehVd materials, recorded and annotated vowel productions, and coded cue extraction. AP coded data analyses and visualization with guidance from TFJ. Both authors contributed to the writing of the manuscript.

# Acknowledgments {-}
We are grateful to OMITTED FOR REVIEW.
<!-- We thank audiences at the Department of Swedish language and multilingualism at Stockholm University and the ExLing 2022 conference in Paris for feedback on earlier presentations of this work. We are particularly grateful to Santiago Barreda for insightful discussion of issues in the evaluation of normalization accounts, Tomas Riad for discussion of Swedish vowel systems, Maryann Tan for collaboration in preparation of the SwehVd database, and to Nathan Young for feedback on segmentation procedures, as well as for access to, and support for, the [SweFA software](https://github.com/mcgarrah/LG-FAVE) for the automatic annotation of Swedish vowels. Finally, we thank Maryann Tan, Xin Xie, Chigusa Kurumada XXXXXXX and other members of the Human Language Processing lab for feedback on earlier versions of the manuscript. -->

# References {#sec:references -}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

```{r, child="SI.Rmd", eval= if (INCLUDE_SI) TRUE else FALSE}
```
